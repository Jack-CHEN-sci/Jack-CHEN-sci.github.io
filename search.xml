<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>An Elaborate Introduction to t-SNE and its Application</title>
      <link href="/2020/04/25/An-Elaborate-Introduction-to-t-SNE-and-its-Application/"/>
      <url>/2020/04/25/An-Elaborate-Introduction-to-t-SNE-and-its-Application/</url>
      
        <content type="html"><![CDATA[<h1 id="t-SNE-算法详解及应用"><a href="#t-SNE-算法详解及应用" class="headerlink" title="t-SNE 算法详解及应用"></a>t-SNE 算法详解及应用</h1><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><h2 id="基本算法：SNE"><a href="#基本算法：SNE" class="headerlink" title="基本算法：SNE"></a>基本算法：SNE</h2><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>在降维过程中保持数据点在原空间中的近邻关系，即高维空间中距离较小的两点投影到低维空间中的距离也应较小，而原空间中距离较大的两点在投影空间中的距离也应较大，从而在低维空间中拟合数据的大致分布结构（特别是聚类结构）。</p><p>直观地理解，两数据点距离越近，也就是该数据点在许多维度上的取值差异越小，说明它们所代表的对象之间的相似度越高。SNE就是将两点间的<strong>欧式距离</strong>转化为一个代表<strong>相似度</strong>的条件概率值，在高维和低维空间中分别进行度量，其中低维空间中最初数据点的位置是随机初始化的，得到两个分布：</p><ul><li>分布1：对输入对象（即原高维数据集中的全体数据点）两两之间的相似度的度量</li><li>分布2：对输出对象（即投影到低维空间中的全体数据点）两两之间相似度的度量</li></ul><p>进而采用“优化”的方法，以<strong>最小化两个分布的差异</strong>为目标，不断迭代。</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><h4 id="度量高维空间中数据点间相似度"><a href="#度量高维空间中数据点间相似度" class="headerlink" title="度量高维空间中数据点间相似度"></a>度量高维空间中数据点间相似度</h4><p>那么如何将数据点间的“距离”在近邻意义下转换为“相似度”呢？怎样使远距离对应低相似度，近距离对应高相似度呢？SNE借用了正态分布的概念来解决这一问题。正态分布是自然界普遍存在的一种概率分布，如果一个事物受到多个变量的影响，不管每个变量本身符合什么分布，它们叠加后，结果的平均值就是正态分布。</p><blockquote><p>正态分布的奇妙之处，就是许多看似随机的事件竟然服从一个公式就能表达的分布，如同上帝之手特意为之一般。</p></blockquote><p><img src="./2020-04-25-An-Elaborate-Introduction-to-t-SNE-and-its-Application/pic/gaussian_distribution.png" alt="图1"></p><p>显然，大型高维数据集在空间中的分布大多可以看作是正态分布，准确地说，任取一个数据点作为“中心”，其他全体数据点到该点的距离应满足正态分布。而正态分布的概率密度恰好符合“到中心点（平均值）的距离越近，概率密度越大”的直觉，SNE就是以正态分布的密度函数为原型，逐步完成对两点间“相似度”的定义。</p><p>正态分布密度函数（其中，$\mu$为正态分布的期望，$\sigma$为正态分布的标准差）：</p><script type="math/tex; mode=display">f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)</script><p>首先，对 $(x-\mu)^{2}/2 \sigma^{2}$ 这一项进行改造，原 $(x-\mu)^{2}$ 表示数据值到期望值的距离平方，按照我们的建模意义改造为 $|x<em>{j}-x</em>{i}|^{2}$，表示任意一点$x_j$到选定的中心点$x_i$的距离平方。相应地，原标准差$\sigma$应改造为以$x_i$为中心的数据点间距离的正态分布的标准差$\sigma_i$（这一项的计算会在下文中详细说明）。得到以下公式：</p><script type="math/tex; mode=display">d_{j|i}^{2}=\frac{\|x_{j}-x_{i}\|^{2}}{2\sigma_{i}^{2}}</script><p>根据其意义，可以理解为任意两点之间的“不相似度”。显然，两点之间的距离越远，不相似度就越高。</p><p>这里我们先对标准差$\sigma_i$有一个大致的理解，其详细的计算过程将在后文讨论。对于数据集中的每一个点，以其为中心的正态分布应该是不同的，因为数据密度在不同位置上是不同的：</p><ul><li>若中心点位于较密集区域（即中心点所在簇内数据点间距较小），为了区别其他簇内数据点，使它们的相似度与中心点所在簇内数据点的相似度产生较大差距，正态分布的标准差$\sigma_i$应当较小，使其密度曲线较“窄”较“高”（如图上半部分）。</li><li>若中心点位于较稀疏区域（即中心点所在簇内数据点间距较大），为了保证如此大间距的簇内数据点仍能共同享有较高的相似度，正态分布的标准差$\sigma_i$应当较大，使其密度曲线较“宽”较“矮”（如图下半部分）。</li></ul><p><img src="./2020-04-25-An-Elaborate-Introduction-to-t-SNE-and-its-Application/pic/dense_vs_sparse.png" alt="图2"></p><p>也就是说计算两点相似度公式中的$\sigma_i$应当视取中心点的不同而不同，若中心点取在数据密度较大的位置处，那么$\sigma_i$应当较小，而若中心点取在数据密度较小的位置处，那么$\sigma_i$应当较大。也正因此，确定不同中心点的$\sigma_i$十分重要，计算方法需要仔细思量。</p><p>接下来，我们回到对任意两点间“相似度”的定义式的设计中。我们改造了原正态分布密度函数中最关键的一部分，并视其为对“不相似度”的度量，那么将改造完成的“不相似度”代入原正态分布密度函数替换原项，得到如下公式，就可以得到我们所期望的“相似度”的定义了吗？</p><script type="math/tex; mode=display">s_{j|i}=\frac{1}{\sqrt{2 \pi} \sigma_i} \exp \left(-\frac{\|x_{j}-x_{i}\|^{2}}{2\sigma_{i}^{2}}\right)</script><p>仔细分析，掐指一算，事情并没有那么简单。两个重要问题浮出水面：</p><ol><li>在计算数据集中任意两点间相似度时，选择哪一个点作为中心点呢？根据我们上文中对$\sigma_i$意义的分析，单纯考虑正态分布的话，似乎选择任何点作为中心点都会更倾向于只是将中心点所在簇与其他簇中的点分开。实际上，采用上式计算得到的结果，是<strong>数据集中任意点与中心点之间的相似度</strong>，中心点所在簇中点的相似度会普遍较高，而其他簇中点的相似度普遍较低。也就是说，仅选取一个中心点用上式计算相似度只会从若干个簇中有效分出一簇（即中心点所在簇），其他簇中点因为相似度相近，仍然混在一起，无法有效区分。</li><li>观察不同标准差的正态分布密度函数图像，不难发现，函数最值的差异会导致密集簇和稀疏簇中两点间的相似度差异。以下图为例：蓝色密集簇中点的相似度可达0.3, 0.35；而橙色稀疏簇中点的相似度只有0.12, 0.15。而这并不是我们所希望发生的情况，因为这样将使得划分簇时没有统一的标准。依然以下图为例：若指定两点间相似度大于0.2的点划归为一簇，那么橙色稀疏簇将无法被划为一簇，相反会被拆解并入其他若干相邻簇中；若指定两点间相似度大于0.1的点划归为一簇，那么蓝色密集簇中将被划入其他簇中的点。理想的结果应该是尽管不同密度的簇选取的标准差$\sigma_i$不同，但一簇内点的相似度应该基本相等。<br><img src="./2020-04-25-An-Elaborate-Introduction-to-t-SNE-and-its-Application/pic/need_of_scaling.png" alt=""></li></ol><p>如何解决这两个问题呢？既然选取一个中心点计算与其他点间相似度，结果为该中心点所在簇中各点的相似度明显高于其他点，也就是能够有效分出该中心点所在的一个簇；那么，我们就可以将数据集中所有点轮流作为中心点,这样一来，每选取一个中心点进行计算便可分出其所在的某个簇，当遍历完数据集中全体数据点后综合分析，就能得到完整的分簇情况。<br>而第二个问题的解决方案在机器学习领域已经相当成熟，称为“<strong>归一化（Normalization）</strong>”，是特征缩放（Feature Scaling）的方法之一。归一化适合对多个实例进行综合对比评价，它将各实例的各项指标的数值分别除以其总和，得到各实例的各项指标的比例水平，从而避免各项指标的具体数值大小对实例间比较的影响，使得不同量纲的的数据具备可比性。现实世界中有不少运用“归一化”的例子，比如在对比不同国家的经济结构或人口结构时，三大产业的具体产值或农村与城市人口的具体数量是不具有可比性的，因为不同国家的经济总量和人口总数是不同的，科学的对比方法应当是计算各产业占国民经济的比重，农村与城市人口占全国总人口的比例。</p><p>综上所述，我们采取<strong>轮流选取所有点作为中心点，分别计算其余点与中心点的（未缩放）相似度，并进行归一化</strong>的策略（用如下公式表达），算得缩放后的点间相似度$p_{j|i}$，以此度量数据在空间中的分布特点。</p><script type="math/tex; mode=display">p_{j|i}=\frac{\exp(-\|x_{i}-x_{j}\|^{2}/2\sigma_{i}^{2})}{\sum_{k \ne i}\exp(-\|x_{i}-x_{k}\|^{2}/2\sigma_{i}^{2})}</script><p>$p<em>{j|i}$可以理解为：$x_i$选$x_j$为邻居的可能性。需要明确：邻居是根据以点$x_i$为中心的正态分布的概率密度，按比例选取的。（在以点$x_i$为中心的正态分布下，距离$x_i$越近的点，概率密度越大，被选择为邻居的可能性就越高。）由于我们只关心两点之间的相似度，且从模型意义上讲，一个点选择自己做自己的近邻这一行为并无道理，所以定义$p</em>{i|i}=0$.</p><p><img src="./2020-04-25-An-Elaborate-Introduction-to-t-SNE-and-its-Application/pic/similarity_matrix.png" alt=""></p><blockquote><p>随机变量x的熵是对随机变量不确定性的度量，是对所有可能发生的事件产生的信息量的期望。<br>随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大。</p></blockquote><p>每一个$\sigma_i$都会对应一个分布$P_i$，$\sigma_i$越大，该分布的熵就越大。SNE在用户选定参数$perplexity$（混乱度）的基础上，使用二分搜索算法寻找$\sigma_i$。</p><h4 id="度量低维空间中数据点间相似度"><a href="#度量低维空间中数据点间相似度" class="headerlink" title="度量低维空间中数据点间相似度"></a>度量低维空间中数据点间相似度</h4><p>对于数据集中的点在低维空间中的投影，</p><h4 id="在低维空间中拟合高维数据分布"><a href="#在低维空间中拟合高维数据分布" class="headerlink" title="在低维空间中拟合高维数据分布"></a>在低维空间中拟合高维数据分布</h4><h2 id="对SNE的改进：t-SNE"><a href="#对SNE的改进：t-SNE" class="headerlink" title="对SNE的改进：t-SNE"></a>对SNE的改进：t-SNE</h2><h3 id="对称化"><a href="#对称化" class="headerlink" title="对称化"></a>对称化</h3><h3 id="拥挤问题"><a href="#拥挤问题" class="headerlink" title="拥挤问题"></a>拥挤问题</h3><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h4 id="t-分布"><a href="#t-分布" class="headerlink" title="t-分布"></a>t-分布</h4><h4 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h4><h2 id="随机近邻嵌入的特点"><a href="#随机近邻嵌入的特点" class="headerlink" title="随机近邻嵌入的特点"></a>随机近邻嵌入的特点</h2><h4 id="非线性"><a href="#非线性" class="headerlink" title="非线性"></a>非线性</h4><h4 id="无监督"><a href="#无监督" class="headerlink" title="无监督"></a>无监督</h4><h4 id="随机性"><a href="#随机性" class="headerlink" title="随机性"></a>随机性</h4><h2 id="算法改进：应用树结构提升效率"><a href="#算法改进：应用树结构提升效率" class="headerlink" title="算法改进：应用树结构提升效率"></a>算法改进：应用树结构提升效率</h2><h2 id="算法应用"><a href="#算法应用" class="headerlink" title="算法应用"></a>算法应用</h2><h3 id="大型高维数据集的可视化"><a href="#大型高维数据集的可视化" class="headerlink" title="大型高维数据集的可视化"></a>大型高维数据集的可视化</h3><h3 id="机器学习过程解释"><a href="#机器学习过程解释" class="headerlink" title="机器学习过程解释"></a>机器学习过程解释</h3>]]></content>
      
      
      
        <tags>
            
            <tag> t-SNE </tag>
            
            <tag> dimensionality reduction </tag>
            
            <tag> projection &amp; embedding </tag>
            
            <tag> manifold learning </tag>
            
            <tag> visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Estimating Graphlet Statistics via Random Walk</title>
      <link href="/2020/03/19/Estimating-Graphlet-Statistics-via-Random-Walk/"/>
      <url>/2020/03/19/Estimating-Graphlet-Statistics-via-Random-Walk/</url>
      
        <content type="html"><![CDATA[<h1 id="A-General-Framework-for-Estimating-Graphlet-Statistics-via-Random-Walk"><a href="#A-General-Framework-for-Estimating-Graphlet-Statistics-via-Random-Walk" class="headerlink" title="A General Framework for Estimating Graphlet Statistics via Random Walk"></a>A General Framework for Estimating Graphlet Statistics via Random Walk</h1><h1 id="一个通过【随机游走】估算【图微元】相关数据的总体框架"><a href="#一个通过【随机游走】估算【图微元】相关数据的总体框架" class="headerlink" title="一个通过【随机游走】估算【图微元】相关数据的总体框架"></a>一个通过【随机游走】估算【图微元】相关数据的总体框架</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>图微元（Graphlet）是图（Graph）的一种非同构诱导子图，通常由$k$个节点构成，其中k∈{3,4,5}. 由于在识别图的局部拓扑结构方面所表现的优质效果，图微元已被应用到多个领域的分析研究中，例如，在线社交网络（Online Social Networks, OSNs）和生物网络中. 但是，对图微元的搜索和计算十分具有挑战性，原因有二. 首先，现实世界中规模巨大的图结构使得对图微元的计算代价极其昂贵；其次，图的拓扑结构并不容易获得，经常需要使用一些应用程序接口（API）从网络上爬取.</p><blockquote><p>注：图微元是作者根据命名单词词根词缀，结合个人理解翻译而来，若有更好翻译版本，烦请赐教。<br>附：名词后缀-let来源于中古法语或拉丁语，大多用于物件名称，表示“小”，例如：booklet——小册子，leaflet——小叶子；有些是科学名词；少部分加在人称名词上，有轻蔑意味。它的引申意义有“微小，在…佩带的小饰物”。</p></blockquote><h2 id="TO-BE-CONTINUED-…-…"><a href="#TO-BE-CONTINUED-…-…" class="headerlink" title="TO BE CONTINUED … …"></a>TO BE CONTINUED … …</h2>]]></content>
      
      
      
        <tags>
            
            <tag> random walk </tag>
            
            <tag> graphlet </tag>
            
            <tag> Markov chain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network Tutorial</title>
      <link href="/2020/02/18/Neural-Network-Tutorial/"/>
      <url>/2020/02/18/Neural-Network-Tutorial/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络（Neural-Network）"><a href="#神经网络（Neural-Network）" class="headerlink" title="神经网络（Neural Network）"></a>神经网络（Neural Network）</h1><p>本文为提纲式笔记，对当前最热门的机器学习算法 —— 深度学习（Deep Learning, DL）的基础，神经网络，进行解析。主要介绍神经网络的数学模型与反向传播算法，重点对反向传播算法的原理、过程和实质进行探究，内容包括：神经网络模型表示、应用举例、反向传播算法的流程与实现。</p><blockquote><p>人工神经网络（Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。</p><p>神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种 <strong>非线性统计性数据建模工具</strong>，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。</p></blockquote><p>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p><h1 id="一-模型表示（Model-Representation）"><a href="#一-模型表示（Model-Representation）" class="headerlink" title="一. 模型表示（Model Representation）"></a>一. 模型表示（Model Representation）</h1><h2 id="1-1-数学模型"><a href="#1-1-数学模型" class="headerlink" title="1.1 数学模型"></a>1.1 数学模型</h2><ul><li>简单来讲，神经元（neurons）就是计算单元，它将输入模块（树突）获取电信号输入（称为“spikes”），并将其引导到输出模块（轴突）</li><li>在我们的模型中：<ul><li>轴突就是输入的特征 x_1, x_2, … , x_n，树突输出的就是假设函数的计算结果</li><li>输入单元 x_0 被称作“<strong>偏差单元</strong>”（bias unit），其值总是“1”</li><li>仍使用在逻辑回归中使用过的 S型(sigmoid)函数 作为假设函数，在神经网络中，通常称为“S型<strong>激活函数</strong>”（sigmoid activation function）</li><li>参数θ通常也被称为“<strong>权重</strong>”</li></ul></li><li>节点与层<ul><li>所有输入节点构成神经网络的第一层，通常称为“<strong>输入层</strong>”（Input layer）</li><li>最终输出假设函数结果的节点构成神经网络的最后一层，通常称为“<strong>输出层</strong>”（output layer）</li><li>输入层与输出层之间的层叫做“<strong>隐藏层</strong>”（hidden layers）</li></ul></li><li>举个例子：<ul><li>下面这张图中，我们将隐藏层中的节点记作a，上标代表该节点处于第几层，下标代表该节点是该层的第几个，并将其称为“<strong>激活单元</strong>”（activation units）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_example.png" width = 25% /></div></li><li>其中，各激活单元进行的是如下计算：（其中 <strong><em>Θ</em></strong> 表示相关的参数矩阵，上标表示该矩阵用于第几层的计算，下标表示矩阵的第几行第几列）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_computation.png" width = 55% /></div></li><li>在本例中，我们用一个 3（下一层中除偏差单元以外的激活单元数目）× 4（该层中包含偏差单元的所有单元数目） 的参数矩阵来计算激活节点，将每一行的参数应用到输入中，以获得一个激活节点的值</li><li>假设函数的输出值取决于 1) 上一层激活节点值之和 与 2) 激活函数</li></ul></li></ul><h2 id="1-2-向量化（Vectorization）"><a href="#1-2-向量化（Vectorization）" class="headerlink" title="1.2 向量化（Vectorization）"></a>1.2 向量化（Vectorization）</h2><ul><li>下面，我们将继续用上面所举的例子，对计算式进行向量化：<ul><li>新定义向量 <strong><em>z</em></strong> ，上标代表节点所在层，下标表示节点在该层中的序号<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_example_replace.png" width = 14% /></div></li><li>也就是说，向量 <strong><em>z</em></strong> 意味着：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vector_z.png" width = 40% /></div></li><li>当向量 <strong><em>x</em></strong> 是隐藏（激活）层 <strong><em>a</em></strong> 时，可将上式写为通式：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vector_z_rewrite.png" width = 20% /></div></li><li>最后，输出层：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/neural_network_hypothesis_vectorized.png" width = 27% /></div></li></ul></li></ul><h2 id="1-3-应用举例（Applications）"><a href="#1-3-应用举例（Applications）" class="headerlink" title="1.3 应用举例（Applications）"></a>1.3 应用举例（Applications）</h2><h3 id="用简单神经网络模拟逻辑门"><a href="#用简单神经网络模拟逻辑门" class="headerlink" title="用简单神经网络模拟逻辑门"></a>用简单神经网络模拟逻辑门</h3><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/simple_nn_individual_example.png" width = 90% /></div><h3 id="用多层神经网络模拟复杂逻辑进行分类"><a href="#用多层神经网络模拟复杂逻辑进行分类" class="headerlink" title="用多层神经网络模拟复杂逻辑进行分类"></a>用多层神经网络模拟复杂逻辑进行分类</h3><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/simple_nn_mixed_example.png" width = 90% /></div><h2 id="1-4-多分类任务"><a href="#1-4-多分类任务" class="headerlink" title="1.4 多分类任务"></a>1.4 多分类任务</h2><p>在多分类任务中，我们有多个输出单元，最终输出的向量长度等于类别数量，且向量中只有一个元素为“1”，代表预测的类别，其他元素为“0”</p><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_multiple_output_oneVSall.png" width = 90% /></div><h1 id="二-代价函数与反向传播"><a href="#二-代价函数与反向传播" class="headerlink" title="二. 代价函数与反向传播"></a>二. 代价函数与反向传播</h1><h2 id="2-1-代价函数（Cost-Function）"><a href="#2-1-代价函数（Cost-Function）" class="headerlink" title="2.1 代价函数（Cost Function）"></a>2.1 代价函数（Cost Function）</h2><ul><li><p>规定以下记号：</p><ul><li><strong><em>L</em></strong>: 网络的总层数</li><li><strong><em>s_l</em></strong>：第 <em>l</em> 层中的节点数量（除去偏差单元）</li><li><strong><em>K</em></strong>：输出层的节点数（即类的数量）</li><li>因为神经网络可能有很多输出单元，我们记<strong><em>h(x)_k</em></strong>为假设函数结果的第k个值</li></ul></li><li><p>公式：</p><ul><li>基于逻辑回归的代价函数<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_logisticReg_costFunction.png" width=80%></div></li><li>改造为神经网络的代价函数<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_cost_function.png" width=85%></div></li></ul></li><li>对比：<ol><li>参数量的增加，由一个假设函数的参数向量<strong><em>θ</em></strong> ，到L-1层神经网络的参数矩阵<strong><em>Θ</em></strong> </li><li>公式的前半部分由一重累加和变为二重累加和，因为要将输出层中的K个单元的计算结果相加</li><li>公式的后半部分由一重累加和变为三重累加和，因为要将L-1层神经网络每层的参数矩阵的所有元素平方相加</li><li>注意：类似于逻辑回归，对常数项θ_0，我们想象为θ_0 * x_0，并且不对其进行正则化；在神经网络中对于每层参数矩阵的第0行第0列不进行正则化</li></ol></li></ul><h2 id="2-2-反向传播算法（Backpropagation）"><a href="#2-2-反向传播算法（Backpropagation）" class="headerlink" title="2.2 反向传播算法（Backpropagation）"></a>2.2 反向传播算法（Backpropagation）</h2><ul><li>注意：反向传播算法并非一个不同于梯度下降的算法，而是梯度下降算法运行在神经网络中时比较有效的计算方法</li><li>反向传播算法（Backpropagation）包括前向传播（Forward pass）和后向传播（Backward pass）两个过程，分别承担不同的计算任务</li><li>链式法则（chain rule）—— 反向传播算法的数学基础  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/chain_rule.png" width=45%></div></li><li>例子：（理解反向传播算法，李宏毅）<ul><li>假设有如下神经网络架构：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_nn.png" width=50%></div></li><li>对其输入层与第二层第一单元之间的关系进行分析：<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute.png" width=50%></div><br>计算 <strong>代价函数</strong> <strong><em>C</em></strong> 对 <strong>参数</strong> <strong><em>w</em></strong> 的导数，根据链式法则，可拆解为计算 <strong>函数</strong> <strong><em>z(w)</em></strong> 对 <strong>参数</strong> <strong><em>w</em></strong> 的导数与 <strong>代价函数</strong> <strong><em>C</em></strong> 对 <strong>变量</strong> <strong><em>z</em></strong> 的导数之积。其中，第一部分可以在 <strong>前向传播</strong> 的过程中直接得到：函数 <strong><em>z(w)</em></strong> 对 参数 <strong><em>w</em></strong> 的导数即为前一层网络中与参数w相关的变量 <strong><em>x</em></strong>。但是第二部分则需要在 <strong>后向传播</strong> 中计算</li><li>后向传播计算过程推导<br>考察下一层神经网络（假设所有神经元的激活函数均为sigmoid函数），如图：<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute1.png" width=70%></div><br>由链式法则，代价函数 <strong><em>C</em></strong> 对 变量 <strong><em>z</em></strong> 的导数可拆解为激活函数 <strong><em>a(z)</em></strong> 对 变量 <strong><em>z</em></strong> 的导数与代价函数 <strong><em>C</em></strong> 对 变量 <strong><em>a</em></strong> 的导数。其中，第一部分由激活函数的形式而确定；而第二部分，鉴于此神经元的计算结果（受参数w影响）参与到了下一层两个神经元的计算中，由链式法则，可以得出如图中最下方计算公式，简单整理后，在最右侧公式中，便可以发现递归计算的身影<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute2.png" width=40%></div><br>通过以上推导过程，我们不难看出：类似于递归算法，若想计算代价函数对某一个参数（第1层中某个单元）的导数，必须先计算代价函数对第2层某个单元的导数，进而必须先计算代价函数对第3层某些(≥2)单元的导数，……，如此层层深入，先得出后面结果，否则无从计算前面。这就是 <strong>后向传播</strong> 的计算过程</li><li>后向传播在网络中的“计算流”<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_nn1.png" width=50%></div></li><li>小结：反向传播的两大过程 —— “前向传播”与“后向传播”<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_summary.png" width=50%></div></li></ul></li><li>算法过程<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_algo.png" width=70%></div><ul><li>导入训练数据集，将神经网络每一层的偏差矩阵 <strong><em>∆^(l)</em></strong> 初始化为零矩阵</li><li>对于m个训练数据，依次进行：<ul><li>将特征向量输入第一层网络（输入层）</li><li>执行前向传播，计算并保存沿途各层的输出结果 <strong><em>a^(l), l = 2, 3, … , L</em></strong></li><li>计算网络输出与真实值的偏差向量 <strong><em>δ^(L)</em></strong></li><li>执行后向传播，计算并保存沿途各层的偏差向量 <strong><em>δ^(l), l = L-1, L-2, … , 2</em></strong>，向量化后得到如下式子：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_delta_partial_compute.png" width=30%></div>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_delta_vectorized.png" width=40%></div></li><li>更新各层偏差矩阵 <strong><em>∆^(l)</em></strong>，向量化公式如下：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_deltaCap_compute.png" width=30%></div></li></ul></li><li>计算代价函数对参数矩阵的偏导数<ul><li>矩阵 <strong><em>D^(l)</em></strong> 用作“累加器”，在计算的过程中把各个分部结果累加起来，最终计算出我们所求的偏导数</li></ul></li></ul></li></ul><h2 id="2-3-反向传播算法实践"><a href="#2-3-反向传播算法实践" class="headerlink" title="2.3 反向传播算法实践"></a>2.3 反向传播算法实践</h2><h3 id="实现细节：参数展开（Unrolling-Parameters）"><a href="#实现细节：参数展开（Unrolling-Parameters）" class="headerlink" title="实现细节：参数展开（Unrolling Parameters）"></a>实现细节：参数展开（Unrolling Parameters）</h3><ul><li>不同于线性回归和逻辑回归，当我们使用神经网络时，参数的数学组织形式不再是向量，而是矩阵<ul><li><strong><em> Θ^(1), Θ^(2), Θ^(3), … </em></strong></li><li><strong><em> D^(1), D^(2), D^(3), … </em></strong></li></ul></li><li>为了能够使用 <strong><em>fminunc()</em></strong> 等高级优化算法，我们需要将以上矩阵“展开”成向量。例如，在Octave/MATLAB中使用以下命令：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thetaVector &#x3D; [ Theta1(:); Theta2(:); Theta3(:); ]</span><br><span class="line">deltaVector &#x3D; [ D1(:); D2(:); D3(:) ]</span><br></pre></td></tr></table></figure></li><li>从“展开”后的向量变回矩阵（假设原矩阵Theta1的大小为10x11, Theta2的大小为10x11，Theta3的大小为1x11）：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 &#x3D; reshape(thetaVector(1:110),10,11)</span><br><span class="line">Theta2 &#x3D; reshape(thetaVector(111:220),10,11)</span><br><span class="line">Theta3 &#x3D; reshape(thetaVector(221:231),1,11)</span><br></pre></td></tr></table></figure></li><li>小节：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/unroll_parameters.png" width=60%></div></li></ul><h3 id="实现细节：梯度检查（Gradient-Checking）"><a href="#实现细节：梯度检查（Gradient-Checking）" class="headerlink" title="实现细节：梯度检查（Gradient Checking）"></a>实现细节：梯度检查（Gradient Checking）</h3><ul><li>作用：检查神经网络反向传播算法的正确性 —— 是否正确计算梯度</li><li>由梯度的几何意义（切线），代价函数J(Θ)对特定参数Θ的偏导数可以计算如下：<ul><li>当只有一个参数矩阵时：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/derivative_approximation.png" width=35%></div></li><li>当有多个参数矩阵时，对Θ_j的偏导数：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/derivative_approximation1.png" width=65%></div></li><li>其中，<strong><em>ε(epsilon)</em></strong> 为任意极小值，一般可以取10^(-4)，若太小会出现数值计算问题</li></ul></li><li>Octave/MATLAB实现：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epsilon &#x3D; 1e-4;</span><br><span class="line">for i &#x3D; 1:n,</span><br><span class="line"> thetaPlus &#x3D; theta;</span><br><span class="line"> thetaPlus(i) +&#x3D; epsilon;</span><br><span class="line"> thetaMinus &#x3D; theta;</span><br><span class="line"> thetaMinus(i) -&#x3D; epsilon;</span><br><span class="line"> gradApprox(i) &#x3D; (J(thetaPlus) - J(thetaMinus))&#x2F;(2*epsilon)</span><br><span class="line">end;</span><br></pre></td></tr></table></figure></li><li>一旦我们计算出gradApprox，与之前计算得到的deltaVector进行比较，若 gradApprox ≈ deltaVector 则说明反向传播算法实现正确</li><li>一旦验证了反向传播算法是正确的，就不需要再次计算gradApprox，因为gradApprox的计算代码运行很慢</li></ul><h3 id="实现细节：随机初始化（Random-Initialization）"><a href="#实现细节：随机初始化（Random-Initialization）" class="headerlink" title="实现细节：随机初始化（Random Initialization）"></a>实现细节：随机初始化（Random Initialization）</h3><ul><li>不同于线性回归和逻辑回归中将参数向量初始化为零向量，在神经网络中，参数矩阵不能初始化为零矩阵，因为这会使得隐藏层的各个单元输入均为零</li><li>而对参数矩阵的其他形式的初始化也不一定行得通，例如下图中的初始化方式 —— 来自同一神经元的联结参数相等（在图中使用同种颜色表示），会导致后面所有神经元在反向传播时的计算结果相同，即所有神经元计算同一函数，捕捉同一特征，使得神经网络“空有其表”，形成巨大冗余<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_wrong_init.png" width=50%></div></li><li>因此，需要对神经网络的参数矩阵进行 <strong>随机初始化</strong>：将其赋值为区间[-ε,ε]中的随机数（注：此处的epsilon与梯度检查中的epsilon没有任何关系）</li><li>Octave/MATLAB实现：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span><br><span class="line">% rand(x,y) is just a function in octave that will initialize </span><br><span class="line">% a matrix of random real numbers between 0 and 1.</span><br><span class="line"></span><br><span class="line">Theta1 &#x3D; rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 &#x3D; rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 &#x3D; rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure></li></ul><h3 id="实现小节（Summary）"><a href="#实现小节（Summary）" class="headerlink" title="实现小节（Summary）"></a>实现小节（Summary）</h3><ul><li>首先，选择一个神经网络架构：共有多少层、每层有多少个神经元 ……<ol><li>输入层单元数 = 特征向量的维度</li><li>输出层单元数 = 类别数量</li><li>隐藏层单元数： 通常情况下越多越好，但是需要权衡随着单元数量增加导致的计算成本增加</li><li>如果隐藏层的数量大于1，那么最好每层的神经元数量是相等的</li></ol></li><li>训练神经网络<ol><li>随机初始化参数（权重）</li><li>执行前向传播，对每个特征向量计算其预测值</li><li>计算代价函数</li><li>执行后向传播，计算梯度/偏导数</li><li>使用梯度检查，确保反向传播算法执行正确，然后关闭检查</li><li>使用梯度下降算法或其他高级优化算法最小化代价函数得到参数值</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> gradient descent </tag>
            
            <tag> neural network </tag>
            
            <tag> backpropagation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression Tutorial</title>
      <link href="/2020/02/14/Logistic-Regression-Tutorial/"/>
      <url>/2020/02/14/Logistic-Regression-Tutorial/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><p>本文为提纲式笔记，在解决线性回归问题的基础上探讨逻辑回归，主要介绍逻辑回归的机器学习算法与正则化技术。内容包括：逻辑回归问题的简介、相关基础术语解析、梯度下降算法、其他高级优化算法的使用、多分类问题解法、正则化技术简介及其在线性回归与逻辑回归中的应用。</p><blockquote><p>逻辑回归是一种对数几率统计模型，其基本形式是使用Logistic函数（一种常见的S型函数）来对二元因变量建模，并且存在着许多复杂的扩展。逻辑回归常用于解决二分类问题（在解决多分类问题时可以使用softmax方法进行处理），用于估计某个事件的可能性。</p><p>逻辑回归与线性回归的关系：逻辑回归与线性回归都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 <strong><em>y</em></strong> 服从伯努利分布，而线性回归假设因变量 <strong><em>y</em></strong> 服从高斯分布。 因此与线性回归有很多相同之处，去除Logistic映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Logistic函数引入了非线性因素，因此可以轻松处理二分类问题。</p></blockquote><h1 id="一-分类问题与逻辑回归"><a href="#一-分类问题与逻辑回归" class="headerlink" title="一. 分类问题与逻辑回归"></a>一. 分类问题与逻辑回归</h1><h2 id="1-1-分类（Classification）"><a href="#1-1-分类（Classification）" class="headerlink" title="1.1 分类（Classification）"></a>1.1 分类（Classification）</h2><ul><li>例子：<ul><li>邮件：垃圾邮件/正常邮件</li><li>线上交易：诈骗/正常</li><li>肿瘤：恶性/良性</li></ul></li><li>分类问题就像回归问题一样，只是我们现在要预测的值只包含少量的离散值。</li><li>典型的<strong>二分类</strong>（binary classification problem）任务：y∈{0，1}<ul><li>y=0时一般表示某种性质的缺失（Negative class）</li><li>y=1时一般表示某种性质的存在（Positive class）</li></ul></li><li>分类任务不能用回归方法解决<ul><li>回归方法：计算出线性回归函数后，映射所有大于0.5的预测作为1，所有小于0.5的预测作为0</li><li>回归方法并不适用，因为分类问题实际上不存在一个能够拟合的线性函数</li></ul></li><li>注意：逻辑回归实际上并非用于回归任务，而是用于分类任务</li></ul><h2 id="1-2-假设函数的表达式（Hypothesis-Representation）"><a href="#1-2-假设函数的表达式（Hypothesis-Representation）" class="headerlink" title="1.2 假设函数的表达式（Hypothesis Representation）"></a>1.2 假设函数的表达式（Hypothesis Representation）</h2><ul><li>需要改变假设函数h(x)的形式以将其输出范围限制在(0, 1)区间内</li><li>做法如下：将theta^T*X放入一个特殊的函数中进行映射  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/sigmoid_function.png" width = 20% /></div></li><li>该函数称为<strong>S型函数(Sigmoid Function)</strong> 或<strong>逻辑函数(Logistic Function)</strong>，图像如下：<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/sigmoid_function_image.png" width = 80% /></div><h3 id="假设函数表达的意义"><a href="#假设函数表达的意义" class="headerlink" title="假设函数表达的意义"></a>假设函数表达的意义</h3></li><li>h(x)将给出输出值为“1”的<strong>概率</strong><ul><li>例如：若h(x)=0.7，则输出值为“1”（具有某种性质）的概率为70% ，而输出值为“0”（不具有某种性质）的概率为30%</li></ul></li><li>数学表达即为：<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/classification_hx_meaning.png" width = 40% /></div><br>  其中，P(y=1|x;θ) 表示在给定假设函数的输入值x及其相关参数值θ后，输出值为1的概率</li></ul><h2 id="1-3-决策边界（Decision-Boundary）"><a href="#1-3-决策边界（Decision-Boundary）" class="headerlink" title="1.3 决策边界（Decision Boundary）"></a>1.3 决策边界（Decision Boundary）</h2><ul><li>为了得到离散的0或1分类，我们可以将假设函数（Sigmoid）的输出转换如下：<ul><li>h(x)≥0.5 -&gt; y=1 即 x≥0 -&gt; y=1</li><li>h(x)<0.5 -> y=0 即 x<0 -> y=0</li></ul></li><li>从图像上来看，假设函数会形成一条决策边界，边界一侧的数据点预测为“1”，另一侧预测为“0”  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/decision_boundary_image.png" width = 80% /></div></li></ul><h2 id="1-4-代价函数（Cost-Function）"><a href="#1-4-代价函数（Cost-Function）" class="headerlink" title="1.4 代价函数（Cost Function）"></a>1.4 代价函数（Cost Function）</h2><ul><li>线性回归的代价函数在分类问题中并不适用<ul><li>因为 1）分类问题中假设函数h(x)的形式很复杂； 2）我们对每一次预测产生的代价cost(h(x),y)的定义为预测与真实标签值之差的平方，平方之后无疑使得整个代价函数更为复杂<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/cost_redefine.png" width = 50% /></div></li><li>事实上，若沿用线性回归的“均方误差”代价函数，则会导致代价函数J(θ)“非凸”（non-convex）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/convex_vs_nonconvex.png" width = 80% /></div></li></ul></li><li>定义适用于逻辑回归的预测代价<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/cost_of_classification.png" width = 65% /></div><ul><li>通过观察图像理解函数意义<ul><li>当实际标签值y=1时：<ul><li>若预测值 h(x) -&gt; 0，则代价 cost -&gt; ∞.（预测失误）</li><li>若预测值 h(x) -&gt; 1，则代价 cost -&gt; 0.（预测正确）</li></ul></li><li>当实际标签值y=0时：<ul><li>若预测值 h(x) -&gt; 0，则代价 cost -&gt; 0.（预测正确）</li><li>若预测值 h(x) -&gt; 1，则代价 cost -&gt; ∞.（预测失误）</li></ul></li></ul></li><li><strong><em>可以理解为我们对算法的预测失误进行处罚，失误越大，处罚越严厉</em></strong></li></ul></li><li>将新定义的 预测代价（cost）代入 代价函数（cost function）中，就可以保证 J(θ) 是凸函数了</li></ul><h2 id="1-5-简化代价函数"><a href="#1-5-简化代价函数" class="headerlink" title="1.5 简化代价函数"></a>1.5 简化代价函数</h2><ul><li>将代价cost(h(x), y)由两种情况的分段函数写为统一形式:<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/calssification_cost.png" width = 60% /></div></li><li>将上式代入代价函数J(θ)中，得到完整的代价函数表达式：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/calssification_cost_function.png" width = 65% /></div></li><li>在实际应用中，进行向量化之后的表达式为：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vectorized_cost_function.png" width = 55% /></div></li></ul><h2 id="1-6-梯度下降"><a href="#1-6-梯度下降" class="headerlink" title="1.6 梯度下降"></a>1.6 梯度下降</h2><ul><li>梯度下降的通式为：<div style="text-indent:5%"><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent_general.png" width = 32% /></div></li><li>计算末尾的微分项后得到：<div style="text-indent:5%"><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/classification_gradient_descent.png" width = 45% /></div></li><li>向量化后得到：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vectorized_classification_gradient_descent.png" width =40% /></div></li></ul><h2 id="1-7-高级优化算法"><a href="#1-7-高级优化算法" class="headerlink" title="1.7 高级优化算法"></a>1.7 高级优化算法</h2><ul><li>优化算法：<ul><li>给定代价函数J(θ)，欲求使得J(θ)最小的参数θ</li><li>算法执行过程中，对于当前的θ，需要计算:<ol><li>J(θ)的函数值</li><li>J(θ)对各个θ_j (j = 0,1,…,n)的导数值</li></ol></li><li>在Octave/MATLAB中：可在编程实现计算以上两者后传参至函数fminunc，fmincg等函数接口中调用高级优化算法</li><li>优化算法举例：<ul><li>梯度下降</li><li>Conjugate Gradient</li><li>BFGS</li><li>L-BFGS</li></ul></li><li>高级优化算法不需要手动调试学习率α，并且通常执行较快；但是算法过程很复杂</li></ul></li></ul><h2 id="1-8-多分类任务：一对多算法（Multiclass-Classification-One-vs-all）"><a href="#1-8-多分类任务：一对多算法（Multiclass-Classification-One-vs-all）" class="headerlink" title="1.8 多分类任务：一对多算法（Multiclass Classification: One-vs-all）"></a>1.8 多分类任务：一对多算法（Multiclass Classification: One-vs-all）</h2><ul><li>举例：<ul><li>邮件标签：工作，朋友，家庭，爱好，…</li><li>医疗诊断：无病，感冒，流感，…</li><li>天气：阴，晴，雨，雪，…</li><li>需要将预测类别从 y∈{0, 1} 二类拓展到 y∈{0, 1, … , n} 多类</li></ul></li><li>假设函数的意义：某数据点属于该类的概率，如果将多分类任务划分为n个二分类任务，那么得到的n个假设函数分别就代表某数据点属于某一类的概率，取这n个概率值中最大的一个为该数据点的预测结果</li><li>一对多算法：<ul><li>以三类为例：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_vs_all.png" width = 100% /></div></li><li>数学表达：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiclass_classification.png" width = 40% /></div></li></ul></li></ul><h1 id="二-正则化（Regularization）"><a href="#二-正则化（Regularization）" class="headerlink" title="二. 正则化（Regularization）"></a>二. 正则化（Regularization）</h1><h2 id="2-1-过拟合（Overfit）"><a href="#2-1-过拟合（Overfit）" class="headerlink" title="2.1 过拟合（Overfit）"></a>2.1 过拟合（Overfit）</h2><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/overfit_vs_underfit.png" width = 90% /></div><ul><li><strong>欠拟合</strong>：算法具有“<strong>高偏差 (High bias)</strong>” —— 假设函数h(x)没有较好地拟合数据，该函数的与数据的趋势很不匹配,它通常是由于函数太简单或使用的特征（变量）太少造成的。</li><li><strong>过拟合</strong>：算法具有“ <strong>高方差 (High variance)</strong> ” —— 假设函数h(x)对数据过度拟合，该函数虽然适合现有数据，但不能很好地 <strong>泛化</strong> 以预测新数据，它通常是由于函数太复杂造成的，会产生许多与数据无关的不必要的曲线和角度。</li><li>解决过拟合问题有两个主要选项：<ol><li>减少特征（变量）的数量<ul><li>手动选择保留哪些特征</li><li>使用某种模型选择算法</li></ul></li><li>正则化<ul><li>保留所有特征，但减少参数θ的大小</li><li>当我们有很多具有微小作用的特征时，正则化将表现非常好</li></ul></li></ol></li></ul><h2 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2.2 代价函数"></a>2.2 代价函数</h2><ul><li>如果假设函数h(x)过拟合，我们可以通过增加代价函数来减少函数中某些变量（特征）的权重</li><li>举个例子：我们想使一个四次函数（如下）形状更趋近于二次函数<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/quartic_function.png" width = 45% /></div><ul><li>那么我们想尽可能地弱化 x^3 和 x^4 的影响，在不去除这些特征，也不改变假设函数形式的前提下，我们可以修改代价函数:<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_cost_function_example.png" width = 70% /></div></li><li>通过在代价函数中加上 θ_3 和 θ_4 相关且对代价函数值影响非常大的项，在降低代价的过程中，我们就必须缩小 θ_3 和 θ_4 ，即将 x^3 和 x^4 的权重降低</li></ul></li><li>通常情况下，我们不知道哪几个特征是需要被降权的，所以，我们可以在一个简单的求和中正则化所有的参数:<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_cost_function.png" width = 60% /></div><ul><li>其中，λ 是正则化参数，它决定了参数的代价扩大了多少</li></ul></li><li>利用上述带有额外一求和项的代价函数，我们可以使假设函数的形式更加平滑以减少过拟合</li><li>注意：如果 λ 太大，它可能会使函数过于平滑并导致欠拟合。</li></ul><h2 id="2-3-正则化的线性回归"><a href="#2-3-正则化的线性回归" class="headerlink" title="2.3 正则化的线性回归"></a>2.3 正则化的线性回归</h2><ul><li>代价函数  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_cost_function.png" width = 60% /></div></li><li>梯度下降<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_GD.png" width = 90% /></div><ul><li>这里将 θ_0 与其他参数 θ_j (j = 1, 2, … , n) 分开处理，我们将 θ_j 的式子变形如下：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_GD_transform.png" width = 65% /></div></li><li>上式中等号右边第一项一般都是小于1的，这就起到了在每次迭代中缩小 θ_j 的作用。注意，此时等号右边第二项与正则化之前一样</li></ul></li><li>标准方程<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_NE.png" width = 45% /></div><br>  方程的大致形式与原来的方程是一样的，只不过在括号里增加了另一项<br>  注意：当 m&lt;n 时，(X^T)X 是不可逆的；但是当我们加入 λL 后，(X^T)X+λL 就是可逆的了</li></ul><h2 id="2-4-正则化的逻辑回归"><a href="#2-4-正则化的逻辑回归" class="headerlink" title="2.4 正则化的逻辑回归"></a>2.4 正则化的逻辑回归</h2><ul><li>代价函数  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_logisticReg_costFunction.png" width = 95% /></div></li><li>梯度下降<ul><li>梯度下降过程与线性回归是一致的，只是式中假设函数 h(x) 不同<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_GD.png" width = 90% /></div></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> gradient descent </tag>
            
            <tag> logistic regression </tag>
            
            <tag> classification </tag>
            
            <tag> regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression Tutorial</title>
      <link href="/2020/02/07/Linear-Regression-Tutorial/"/>
      <url>/2020/02/07/Linear-Regression-Tutorial/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h1><p>本文为提纲式笔记，以线性回归问题作为机器学习的入门研究对象，主要介绍一元和多元线性回归的机器学习算法，并提及多项式回归问题和标准方程解法。内容包括：线性回归问题的简介、相关基础术语解析、梯度下降算法介绍与应用、标准方程解法对比。</p><blockquote><p>在统计学中，<strong>线性回归</strong> 是利用称为线性回归方程的最小二乘函数（最小化 <strong>误差的平方和</strong> 寻找数据的最佳函数匹配）对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为一元回归或简单回归，大于一个自变量情况的叫做多元回归。（Wikipedia）</p></blockquote><p>线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。</p><h1 id="一-一元线性回归"><a href="#一-一元线性回归" class="headerlink" title="一. 一元线性回归"></a>一. 一元线性回归</h1><h2 id="1-1-一些记号"><a href="#1-1-一些记号" class="headerlink" title="1.1 一些记号"></a>1.1 一些记号</h2><ul><li>m：训练样本规模</li><li>x：输入变量/特征</li><li>y：输出变量/目标变量</li><li>(x,y)：一个训练样本</li><li>h：hypothesis（假设）—— 算法学习到的函数</li></ul><h2 id="1-2-代价函数"><a href="#1-2-代价函数" class="headerlink" title="1.2 代价函数"></a>1.2 代价函数</h2><ul><li>用代价函数来衡量假设函数h的准确性：它取假设函数h对输入x的所有结果h(x)与和实际输出y的平均值之差(实际上是平均值的更fancy的版本)。</li><li><strong>平方误差函数</strong>（squared error function）或<strong>均方误差</strong>（mean squared error）。</li></ul><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/mean_squared_error.png" width = 60% /></div><ul><li>式中取均值后再乘以1/2，是因为它更够使得梯度下降的计算更加方便，因为平方式求导后的系数会将其抵消。</li></ul><h2 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h2><ul><li>过程类似：下山（二维情况，如下图）。在当前位置环视四周，寻找当前最快的下山路径，不断重复。最终，山的最低点就是代价函数的最小值</li></ul><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent.png" width = 60% /></div><ul><li>做法：对代价函数求导，该点的导数就是山的坡度，也就是移动的方向。代价函数沿着当前最陡的下降方向移动，移动的步长通过参数\alpha —— <strong>学习率</strong>（learning rate）—— 决定。</li><li>学习率决定下降的步长，代价函数的导数决定下降的方向</li><li>不同起始点出发后最后到达的最低点可能不同</li><li>算法：</li></ul><p style="text-indent:30%"> repeat until convergence:</p><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent_algo.png" width = 30% /></div><p style="text-indent:30%"> where j=0,1 represents the feature index number.</p><ul><li>注意：对于每一轮迭代，参数的更新应该是同步的：在计算完毕后再统一更新；在有其他参数未完成计算之前更新某一已完成计算的参数会造成错误</li></ul><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent_implement.png" width = 70% /></div><ul><li>关于学习率（lr）<ul><li>若lr太小，下降速度会很慢</li><li>若lr太大，下降过程中可能会越过最低点，无法收敛，甚至会发散</li><li>梯度下降算法为什么在固定lr的情况下仍然能收敛？<ul><li>在接近最低点的过程中，代价函数的导数项会逐渐变小，使得移动步长逐渐变小</li></ul></li></ul></li><li><strong>批量梯度下降</strong> （Batch Gradient Descent）<ul><li>下降的每一步都用上所有的训练数据（全集）</li><li>反映在一元线性回归中：每次都是计算累加值</li></ul></li></ul><h1 id="二-多元线性回归"><a href="#二-多元线性回归" class="headerlink" title="二. 多元线性回归"></a>二. 多元线性回归</h1><h2 id="2-1-一些记号"><a href="#2-1-一些记号" class="headerlink" title="2.1 一些记号"></a>2.1 一些记号</h2><ul><li>m：训练样本规模</li><li>n：变量/特征/维度数</li><li>x^(i)：第i个训练样本的特征向量</li><li>x^(i)_j：第i个训练样本的特征向量的第j个特征</li><li>h：hypothesis（假设）—— 算法学习到的函数</li></ul><h2 id="2-2-回归函数"><a href="#2-2-回归函数" class="headerlink" title="2.2 回归函数"></a>2.2 回归函数</h2><ul><li><p>多元线性回归拟合（假设）函数形式：</p>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_function.png" width = 60% /></div><ul><li>x_1, x_2, … , x_n 表示n个变量（特征）</li><li>theta_0, theta_1, … , theta_n 为需要计算的参数</li></ul></li><li><p>对任意 i ∈ 1,…,m，x^(i)_0 = 1. 那么将上式向量化后写为：</p><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_function_vector.png" width = 60% /></div></li></ul><h2 id="2-3-代价函数"><a href="#2-3-代价函数" class="headerlink" title="2.3 代价函数"></a>2.3 代价函数</h2><ul><li>与一元线性回归的代价函数形式相同，均采用<strong>平方误差函数</strong>：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/cost_function.png" width = 45% /></div></li></ul><h2 id="2-4-梯度下降算法"><a href="#2-4-梯度下降算法" class="headerlink" title="2.4 梯度下降算法"></a>2.4 梯度下降算法</h2><ul><li>算法实质与一元线性回归一致，表达形式近乎一致：&lt;/br&gt;<br>  <img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_GDAlgo.png" width = 70% /><ul><li>对于参数theta_j的计算：累加的是m个训练样本（特征向量）在当前预测函数h(x)下的预测值与实际值之差的平方和对第j个变量（特征）的导数</li><li>全部n+1个参数 theta_0, theta_1, … , theta_n 计算完成后再更新</li></ul></li><li>将其各项逐个写出即为：&lt;/br&gt;<br>  <img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_GDAlgo(1).png" width = 55% /></li></ul><h2 id="2-5-梯度下降的加速方法（tricks）"><a href="#2-5-梯度下降的加速方法（tricks）" class="headerlink" title="2.5 梯度下降的加速方法（tricks）"></a>2.5 梯度下降的加速方法（tricks）</h2><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><ul><li>将输入的数值规范为同样大小的规模，避免梯度下降过程中的反复震荡&lt;/br&gt;</li><li>将输入值除以输入变量的范围(即最大值减去最小值)，得到的新范围在-1到1之间。<br><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/feature_scaling.png" width = 100% /></li><li>作用：通过减少梯度下降的迭代次数从而起到加速作用<ul><li>注：不能解决梯度下降陷入局部最优解的问题<h3 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h3></li></ul></li><li>均值归一化是特征缩放的具体方法之一</li><li>操作如下：<ol><li>从每一输入变量的值中减去全体输入变量的平均值，从而得到一个平均值为0的新输入变量。</li><li>将新输入变量除以样本标准差</li></ol></li><li>遵循如下公式：&lt;/br&gt;<br>  <img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/mean_normalization.png" width = 30% />&lt;/br&gt;<br>  式中ui为所有变量i的值的平均值，si为变量i的值的范围（max-min）或标准差（standard deviation）</li></ul><h2 id="2-6-怎样选择合适的“学习率α”？"><a href="#2-6-怎样选择合适的“学习率α”？" class="headerlink" title="2.6 怎样选择合适的“学习率α”？"></a>2.6 怎样选择合适的“学习率α”？</h2><ul><li><strong>调试梯度下降</strong>：以迭代次数为x轴，代价函数J(theta)的值为y轴画出变化图。如果J(theta)一直在增长，那么应当将学习率减小</li><li><strong>自动收敛判断</strong>：若代价函数J(theta)在一次迭代中减少的值小于E（E是一个很小很小的值，例如0.001），那么可以判定J(theta)收敛了。但是在实际应用中选择合适的阈值E是非常困难的</li><li>已经证明：如果学习率α足够小,那么J(θ)将在每次迭代中都减少</li></ul><h1 id="三-自定义特征与多项式回归"><a href="#三-自定义特征与多项式回归" class="headerlink" title="三. 自定义特征与多项式回归"></a>三. 自定义特征与多项式回归</h1><p>我们可以将多个特征组合为一个，例如将特征x1, x2通过乘法运算x1*x2组合为新特征x3</p><h2 id="3-1-多项式回归"><a href="#3-1-多项式回归" class="headerlink" title="3.1 多项式回归"></a>3.1 多项式回归</h2><ul><li>若线性回归无法较好的拟合数据的分布，那么假设函数h(x)就需要是非线性的</li><li>可以通过将假设函数h(x)变成二次函数、三次函数或平方根函数(或任何其他形式)来改变曲线形状</li><li>注意： <strong>特征缩放！</strong><ul><li>平方和开方运算会较大地改变特征（变量）的取值范围</li><li>例如，x1∈(1,1000)，那么平方后x1^2∈(1,1000000)，立方后x1^3∈(1,1000000000)，开方后√x1∈(1,32)</li></ul></li></ul><h1 id="四-标准方程"><a href="#四-标准方程" class="headerlink" title="四. 标准方程"></a>四. 标准方程</h1><h2 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h2><ul><li>与梯度下降的作用一样，用于寻找使得代价函数J(theta)最小的theta值</li><li><strong>标准方程</strong>（normal equation）提供了一种高效的方法，可以直接解出theta</li><li>与梯度下降不同，标准方程不是一个迭代算法，而是通过分别对theta_j（每一个j）求导数并令其等于0求解得到。方程如下:<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/normal_equation.png" width = 30% /></div><br>  其中，X为m×(n+1)的矩阵，m行：每行为某个样本的特征向量，n+1列：n个特征 + 1个常数项因子（“1”）</li><li>使用标准方程的话无需对特征进行缩放</li></ul><h2 id="4-2-梯度下降VS标准方程"><a href="#4-2-梯度下降VS标准方程" class="headerlink" title="4.2 梯度下降VS标准方程"></a>4.2 梯度下降VS标准方程</h2><div class="table-container"><table><thead><tr><th style="text-align:center">梯度下降</th><th style="text-align:center">标准方程</th></tr></thead><tbody><tr><td style="text-align:center">需要选择学习率α</td><td style="text-align:center">无需对学习率α进行选择</td></tr><tr><td style="text-align:center">多次迭代</td><td style="text-align:center">没有迭代过程</td></tr><tr><td style="text-align:center">复杂度为O(k*n^2)</td><td style="text-align:center">复杂度为O(n^3)，需要计算矩阵(X^T)*X的逆</td></tr><tr><td style="text-align:center">算法在n非常大时表现稳定</td><td style="text-align:center">如果n非常大，那么算法执行时间会非常长</td></tr></tbody></table></div><ul><li>注：在实践中，当特征数n超过10,000时，可以考虑从标准方程法转为使用梯度下降。</li></ul><h2 id="4-3-不可逆性"><a href="#4-3-不可逆性" class="headerlink" title="4.3 不可逆性"></a>4.3 不可逆性</h2><ul><li>线性代数中，并非所有矩阵都是可逆的，我们将可逆矩阵称为奇异（singular）矩阵或退化（degenerate）矩阵</li><li>Octave/MATLAB中<ul><li>函数pinv()：pseudo-inverse 伪逆（一般使用此函数可以得到想求的theta）</li><li>函数inv()：inverse 逆</li></ul></li><li>什么情况下X^T*X不可逆？<ol><li>存在冗余特征（线性相关的）<ul><li>例如：在预测房价时，特征x_1为房子的面积，单位为feet^2，特征x_2为房子的面积，单位为m^2</li><li>因为1m = 3.28feet，故x_1 将恒等于 (3.28)^2 * x_2</li></ul></li><li>过量特征（e.g. 样本数量 ≤ 特征数量）<ul><li>解决方案：删除一部分特征 或 使用正则化（regularization）的方法</li></ul></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> linear regression </tag>
            
            <tag> gradient descent </tag>
            
            <tag> normal equation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-Theme-Snail</title>
      <link href="/2019/11/01/Hexo-Theme-Snail/"/>
      <url>/2019/11/01/Hexo-Theme-Snail/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo-theme-snail"><a href="#hexo-theme-snail" class="headerlink" title="hexo-theme-snail"></a>hexo-theme-snail</h1><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">View Hexo-Theme-Snail Sources On Github &#10174; </a></p><p><a href="https://www.dusign.net" target="_blank" rel="noopener">View Live Super Snail Blog &#10174;</a></p><p><img src="snail.png" alt="hexo-theme-snail"></p><p>Hexo-theme-snail is a succinct hexo theme. It has two colors, light and star, that can be set according to your own preferences in the settings, and also has the functions of sharing and commenting. More features are under development.</p><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li>light color theme and star theme</li><li>diversified comment system</li><li>notice tips</li><li>share to other platforms (under development)</li><li>picture sharing (under development)</li></ul><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a>Install Hexo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br></pre></td></tr></table></figure><h3 id="Setup-your-blog"><a href="#Setup-your-blog" class="headerlink" title="Setup your blog"></a>Setup your blog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br></pre></td></tr></table></figure><h3 id="Installation-Theme"><a href="#Installation-Theme" class="headerlink" title="Installation Theme"></a>Installation Theme</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> blog</span><br><span class="line">$ rm -rf <span class="built_in">source</span></span><br><span class="line">$ rm _config.yml package.json README.md LICENSE</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/dusign/hexo-theme-snail.git</span><br><span class="line">$ mv ./hexo-theme-snail/snail ./themes</span><br><span class="line">$ mv ./hexo-theme-snail/* ./</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure><h3 id="Set-Theme"><a href="#Set-Theme" class="headerlink" title="Set Theme"></a>Set Theme</h3><p>Modify the value of <code>theme:</code> in <code>_config.yml</code><br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br></pre></td></tr></table></figure></p><h3 id="Start-the-Server"><a href="#Start-the-Server" class="headerlink" title="Start the Server"></a>Start the Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><h3 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h3><p>Replace the following information with your own.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> </span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">At</span> <span class="string">the</span> <span class="string">bottom</span> <span class="string">of</span> <span class="string">the</span> <span class="string">well,</span> <span class="string">it</span> <span class="string">is</span> <span class="string">destined</span> <span class="string">to</span> <span class="string">see</span> <span class="string">only</span> <span class="string">the</span> <span class="string">sky</span> <span class="string">at</span> <span class="string">the</span> <span class="string">wellhead.</span> </span><br><span class="line">          <span class="string">However,</span> <span class="string">the</span> <span class="string">starting</span> <span class="string">point</span> <span class="string">only</span> <span class="string">affects</span> <span class="string">the</span> <span class="string">process</span> <span class="string">of</span> <span class="string">reaching</span> <span class="string">your</span> <span class="string">peak</span> <span class="string">and</span> <span class="string">does</span> <span class="string">not</span> <span class="string">determine</span> <span class="string">the</span> <span class="string">height</span> <span class="string">you</span> <span class="string">reach.</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Dusign</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">en</span></span><br><span class="line"><span class="attr">timezone:</span></span><br></pre></td></tr></table></figure></p><h3 id="Site-Settings"><a href="#Site-Settings" class="headerlink" title="Site Settings"></a>Site Settings</h3><p>Put customized pictures in <code>img</code> directory.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site settings</span></span><br><span class="line"><span class="attr">SEOTitle:</span> <span class="string">Hexo-theme-snail</span></span><br><span class="line"><span class="attr">email:</span> <span class="string">hexo-theme-snail@mail.com</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"A hexo theme"</span></span><br><span class="line"><span class="attr">keyword:</span> <span class="string">"dusign, hexo-theme-snail"</span></span><br><span class="line"><span class="attr">header-img:</span> <span class="string">img/header_img/home-bg-1-dark.jpg</span></span><br><span class="line"><span class="attr">signature:</span> <span class="literal">true</span> <span class="comment">#show signature</span></span><br><span class="line"><span class="attr">signature-img:</span> <span class="string">img/signature/Just-do-it-white.png</span></span><br></pre></td></tr></table></figure></p><h3 id="SNS-Settings"><a href="#SNS-Settings" class="headerlink" title="SNS Settings"></a>SNS Settings</h3><p>If you don’t want to display it, you can delete it directly.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SNS settings</span></span><br><span class="line"><span class="attr">github_username:</span>    <span class="string">dusign</span></span><br><span class="line"><span class="attr">twitter_username:</span>   <span class="string">dusignr</span></span><br><span class="line"><span class="attr">facebook_username:</span>  <span class="string">Gang</span> <span class="string">Du</span></span><br><span class="line"><span class="attr">zhihu_username:</span> <span class="string">dusignr</span></span><br></pre></td></tr></table></figure></p><h3 id="Sidebar-Settings"><a href="#Sidebar-Settings" class="headerlink" title="Sidebar Settings"></a>Sidebar Settings</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sidebar Settings</span></span><br><span class="line"><span class="attr">sidebar:</span> <span class="literal">true</span>                      <span class="comment"># whether or not using Sidebar.</span></span><br><span class="line"><span class="attr">sidebar-about-description:</span> <span class="string">"Welcome to visit, I'm Dusign!"</span></span><br><span class="line"><span class="attr">sidebar-avatar:</span> <span class="string">img/ironman-draw.png</span>      <span class="comment"># use absolute URL, seeing it's used in both `/` and `/about/`</span></span><br><span class="line"><span class="attr">widgets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">featured-tags</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">short-about</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">recent-posts</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">friends-blog</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">archive</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">category</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># widget behavior</span></span><br><span class="line"><span class="comment">## Archive</span></span><br><span class="line"><span class="attr">archive_type:</span> <span class="string">'monthly'</span></span><br><span class="line"><span class="attr">show_count:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Featured Tags</span></span><br><span class="line"><span class="attr">featured-tags:</span> <span class="literal">true</span>                     <span class="comment"># whether or not using Feature-Tags</span></span><br><span class="line"><span class="attr">featured-condition-size:</span> <span class="number">1</span>              <span class="comment"># A tag will be featured if the size of it is more than this condition value</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Friends</span></span><br><span class="line"><span class="attr">friends:</span> <span class="string">[</span></span><br><span class="line">    <span class="string">&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Blog"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"https://blog.csdn.net/d_Nail"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Web"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"#"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Github"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"https://github.com/dusign"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Other"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"#"</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"><span class="string">]</span></span><br></pre></td></tr></table></figure><h3 id="Theme"><a href="#Theme" class="headerlink" title="Theme"></a>Theme</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br></pre></td></tr></table></figure><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">      <span class="attr">github:</span> <span class="string">github.repository.address</span></span><br><span class="line">      <span class="attr">coding:</span> <span class="string">coding.repository.address</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><h3 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h3><p>See httpymls://github.com/imsun/gitment for detailed configuration method.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Comment</span></span><br><span class="line"><span class="comment">## This comment system is gitment</span></span><br><span class="line"><span class="comment">## gitment url: https://github.com/imsun/gitment</span></span><br><span class="line"><span class="attr">comment:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">owner:</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">  <span class="attr">client_id:</span></span><br><span class="line">  <span class="attr">client_secret:</span></span><br></pre></td></tr></table></figure></p><h3 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tip</span></span><br><span class="line"><span class="attr">tip:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">content:</span> <span class="string">欢迎访问</span> <span class="string">&lt;a</span> <span class="string">href="https://www.dusign.net"</span> <span class="string">target="dusign"&gt;dusign&lt;/a&gt;</span> <span class="string">的博客，博客系统一键分享的功能还在完善中，请大家耐心等待。</span></span><br><span class="line">          <span class="string">若有问题或者有好的建议欢迎留言，笔者看到之后会及时回复。</span></span><br><span class="line">          <span class="string">评论点赞需要github账号登录，如果没有账号的话请点击</span> </span><br><span class="line">          <span class="string">&lt;a</span> <span class="string">href="https://github.com"</span> <span class="string">target="view_window"</span> <span class="string">&gt;</span> <span class="string">github</span> <span class="string">&lt;/a&gt;</span> <span class="string">注册，</span> <span class="string">谢谢</span> <span class="string">!</span></span><br></pre></td></tr></table></figure><h3 id="Color-Sheme"><a href="#Color-Sheme" class="headerlink" title="Color Sheme"></a>Color Sheme</h3><p>Set the <code>enable</code> value of the desired color sheme to <code>true</code>. If the value of <code>bg_effects.star.enable</code> is <code>true</code>, please modify the value of <code>highlight_theme</code> in <code>./themes/snail/_config.yml</code> to <code>night</code>.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Color Sheme</span></span><br><span class="line"><span class="comment">## If there is no effect after modification, please empty the cache and try again.</span></span><br><span class="line"><span class="comment">## ⚠️ The following special effects will take up a lot of cpu resorces, please open it carefully.</span></span><br><span class="line"><span class="attr">bg_effects:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">color:</span> <span class="number">129</span><span class="string">,200,61</span></span><br><span class="line">    <span class="attr">pointColor:</span> <span class="number">129</span><span class="string">,200,61</span></span><br><span class="line">    <span class="attr">opacity:</span> <span class="number">0.7</span></span><br><span class="line">    <span class="attr">zIndex:</span> <span class="number">-9</span></span><br><span class="line">    <span class="attr">count:</span> <span class="number">99</span></span><br><span class="line">  <span class="attr">mouse_click:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">content:</span> <span class="string">'"🌱","just do it","🌾","🍀","don'</span><span class="string">'t give up","🍂","🌻","try it again","🍃","never say die","🌵","🌿","🌴"'</span></span><br><span class="line">    <span class="attr">color:</span> <span class="string">'"rgb(121,93,179)"</span></span><br><span class="line"><span class="string">          ,"rgb(76,180,231)"</span></span><br><span class="line"><span class="string">          ,"rgb(184,90,154)"</span></span><br><span class="line"><span class="string">          ,"rgb(157,211,250)"</span></span><br><span class="line"><span class="string">          ,"rgb(255,0,0)"</span></span><br><span class="line"><span class="string">          ,"rgb(242,153,29)"</span></span><br><span class="line"><span class="string">          ,"rgb(23,204,16)"</span></span><br><span class="line"><span class="string">          ,"rgb(222,0,0)"</span></span><br><span class="line"><span class="string">          ,"rgb(22,36,92)"</span></span><br><span class="line"><span class="string">          ,"rgb(127,24,116)"</span></span><br><span class="line"><span class="string">          ,"rgb(119,195,79)"</span></span><br><span class="line"><span class="string">          ,"rgb(4,77,34)"</span></span><br><span class="line"><span class="string">          ,"rgb(122,2,60)"'</span></span><br><span class="line">  <span class="attr">star:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h2 id="Releases"><a href="#Releases" class="headerlink" title="Releases"></a>Releases</h2><p>V1.0</p><ul><li>fix the bugs</li><li>add comment system</li><li>add notice tips</li><li>add star sheme</li></ul><h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p>Apache License 2.0 Copyright(c) 2018-2020 <a href="https://github.com/dusign" target="_blank" rel="noopener">Dusign</a>   </p><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">hexo-theme-snail</a> is derived from <a href="https://github.com/Huxpro/huxpro.github.io" target="_blank" rel="noopener">Huxpro</a> Apache License 2.0. Copyright (c) 2015-2020 Huxpro</p>]]></content>
      
      
      
        <tags>
            
            <tag> dusign </tag>
            
            <tag> hexo-theme-snail </tag>
            
            <tag> snail </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
