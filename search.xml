<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Answering Questions about Charts and Generating Visual Explanations</title>
      <link href="/2020/06/08/Answering-Questions-about-Charts-and-Generating-Visual-Explanations/"/>
      <url>/2020/06/08/Answering-Questions-about-Charts-and-Generating-Visual-Explanations/</url>
      
        <content type="html"><![CDATA[<h1 id="图表问题的回答及解释自动生成系统"><a href="#图表问题的回答及解释自动生成系统" class="headerlink" title="图表问题的回答及解释自动生成系统"></a>图表问题的回答及解释自动生成系统</h1><p><strong>CHI 2020 论文《Answering Questions about Charts and Generating Visual Explanations》阅读报告</strong></p><h2 id="一-摘要"><a href="#一-摘要" class="headerlink" title="一. 摘要"></a>一. 摘要</h2><p>人们经常使用图表(chart)进行数据分析，回答数据问题，并向他人解释自己的回答。经过调查，我们发现人们对图表提出的问题和给出解释常常涉及到图表中的可视特征（如形状、颜色、宽高等）。基于这些，我们开发了一套自动的图表问答流水线，在给出针对某幅图表的某个问题的回答的同时，能够对得出答案的过程生成指向可视信息的自然语言解释。</p><p>输入的 Vega-Lite 格式图表和一个关于该图表的自然语言描述的问题，我们的系统首先从图表中提取数据和可视编码；然后它会对自然语言问题进行转译，将问题描述中对图表可视特征的引用转换为对数据域和数据值的引用；进而采用一种目前最先进的机器学习算法对转译后的问题做出回答。最后，系统使用了一个基于模板的方法，生成该回答的自然语言解释，说明其答案是怎样利用图表的可视信息得出的。</p><p>我们进行了用户调研，结果表明我们的系统所生成的“可视解释”在透明性（transparency，指文字解释的详细与易懂程度）方面明显高于人工生成的解释，并且在实用性和可信度方面与人工生成的解释相当。</p><h2 id="二-背景介绍"><a href="#二-背景介绍" class="headerlink" title="二. 背景介绍"></a>二. 背景介绍</h2><p>使用可视化的方法分析数据，回答问题并解释答案的得出过程是许多决策任务的核心环节。然而，执行如此复杂的可视分析任务并非易事，用户经常会面对一些复杂问题，而回答这些问题常常需要综合多种操作或运算，例如从图表中检索数值、查找极值、比较与整合多个部分的数据，还有求和与求差。</p><h3 id="2-1-问答示例"><a href="#2-1-问答示例" class="headerlink" title="2.1 问答示例"></a>2.1 问答示例</h3><p>考虑下图中的柱状图以及问题“<em>哪个宗教被大多数牧师认为宗教极端主义最为盛行?</em>”</p><p><img src="./fig/religious_extremism.png" alt="Religious Extremism"></p><p>为了回答这个问题，用户需要在知晓柱形与色彩这些可视特征所代表的意义的基础上，从视觉上比较橙色柱所代表的数值，即找出最长的那根橙色柱，然后从图中找到对应的宗教名称；对于这个问题，最终答案是“Muslims”。</p><h3 id="2-2-“可视型”问题与提问习惯"><a href="#2-2-“可视型”问题与提问习惯" class="headerlink" title="2.2 “可视型”问题与提问习惯"></a>2.2 “可视型”问题与提问习惯</h3><p>当用户在分析一个图表时，他们经常会通过引用图表的可视特征（例如柱形与色彩）以及这些特征的可视属性（例如柱宽和色值）提出问题。举个例子，在分析上图中的柱状图时，用户可能会问“<em>哪个宗教的橙色柱最长？</em>”，这个问题与我们示例中的问题两个问题是等价的，可以称作其“可视版本”。尽管它仍是一个复杂问题（需要综合查找与比较两个操作），但是因为它引用了图表中的可视特征，使得它更短，更直接地暗示了系统回答问题所需要进行的操作。</p><p>然而，要回答类似这样的复杂问题，不论它们引用图表的可视特征与否，都可能会耗费极长的时间和精力，因为回答者需要执行若干复杂的操作才能最终得出答案。</p><p>为了更深入地了解人们针对图表的提问习惯，我们进行了一个调查，收集了52个真实世界中的图表，以及与之相关的629个人工提出的问题和748个人工生成的解释。我们之后对这些问题按照两个正交维度进行分类；<br>（1）<strong>查找型</strong>（回答问题仅需要一次数值检索操作）— <strong>复合型</strong>（回答问题需要综合多种操作）；<br>（2）<strong>可视型</strong>（语言中引用了图表的可视特征）— <strong>非可视型</strong>（语言中未引用图表的可视特征）；<br>我们发现：人们经常会问复合型问题（70%）和可视型问题（12%），可视型的解释非常常见（51%）。</p><h3 id="2-3-自动问答系统"><a href="#2-3-自动问答系统" class="headerlink" title="2.3 自动问答系统"></a>2.3 自动问答系统</h3><p>我们能否开发一个工具来自动地回答此类关于图表的自然语言问题？自动问答可以为用户提供多方面的便利，它会极大地节约用户执行如检索，比较，整合（求和，求均值）等复杂操作的时间和精力。更为重要的是，将这样一个自然语言交互界面引入数据分析的工作流中可以降低使用图表分析数据的门槛。使那些未曾接受过数据分析工具和可视化知识培训的人们轻松得到问题的答案。然而，对于那些依赖于这样一个自动工具的用户，有一点至关重要，那就是工具必须能够透明地解释它如何得出答案的，以此方便用户知晓操作过程以及验证结果的正确性。</p><p>此外，我们的调查表明大部分好的解释是“可视型”的，因为它们能够解释答案是怎样从图表的可视特征中提取出来的。但是，目前还没有工作在图表的自动问答系统中提供答案的解释。</p><p>本文中，我们提出一个回答图表的自然语言问题并能够自动生成对应可视解释的流水线。我们的系统建立在 <strong><em>Sempre</em></strong> 的基础上，Sempre是一个针对关系型数据表的问答系统，主要回答复合、非可视型的问题。</p><p>我们极大地拓展了Sempre，使其可以回答关于图表的问题并给出对应的可视型解释。我们的流水线可以处理查找型、组合型与可视型、非可视型问题的任意交叉。我们方法的关键在于利用Vega-Lite格式的输入图表的可视编码结构（Vega-Lite格式是一种图表描述范式，描述了数据映射的可视编码）以准确地回答可视型问题并生成对应的可视解释。</p><p>我们以在调查中收集到的629个图表问题为语料库，对我们的问答系统进行了评估。发现我们的系统正确回答了语料库中的所有问题的51%，而单用Sempre只能获得39%的准确率，二者相差12%。对于可视问题，采用我们的系统之后正确率更是超过了53%；对于非可视问题，我们的系统也比Sempre的表现高出了6%。这些结果表明，图表的可视编码信息对于图表问题的自动回答是非常有价值的。</p><p>最后，我们进行了一项用户调查，发现我们的系统生成的可视化解释明显比人工生成的解释更加透明，同时在可用性和可信度方面与人工生成的解释不相上下。</p><h2 id="三-方法"><a href="#三-方法" class="headerlink" title="三. 方法"></a>三. 方法</h2><h3 id="3-1-系统概况"><a href="#3-1-系统概况" class="headerlink" title="3.1 系统概况"></a>3.1 系统概况</h3><p><strong>输入</strong>：一张Vega-Lite格式<strong>图表(chart)</strong>和 一个自然语言<strong>问题(question)</strong><br><strong>输出</strong>：问题<strong>答案(answer)</strong>和 答案<strong>解释(explanation)</strong></p><blockquote><p>示例：<br>输入图表为图一柱状图，问题为 “Which religion has the longest orange component?”；<br>输出问题答案为 “Muslims”，解释为 “I looked up ‘Religion’ of the longest orange bar.“。</p></blockquote><h4 id="系统流水线"><a href="#系统流水线" class="headerlink" title="系统流水线"></a>系统流水线</h4><p><img src="./fig/pipeline.png" alt="Pipeline"></p><h5 id="阶段一：图表转换-Chart-table-transformation"><a href="#阶段一：图表转换-Chart-table-transformation" class="headerlink" title="阶段一：图表转换 (Chart-table transformation)"></a>阶段一：图表转换 (Chart-table transformation)</h5><ol><li>将输入的图表转化为 Vega-Lite 格式</li><li>从转化后的 Vega-Lite 格式中提取可视编码信息</li><li>将提取的信息展开成一张关系型数据表</li></ol><h5 id="阶段二：问题转译与答案生成-Question-transformation-amp-Answer-generation"><a href="#阶段二：问题转译与答案生成-Question-transformation-amp-Answer-generation" class="headerlink" title="阶段二：问题转译与答案生成 (Question transformation &amp; Answer generation)"></a>阶段二：问题转译与答案生成 (Question transformation &amp; Answer generation)</h5><ol><li>依照阶段一提取的数据关系信息，将问题中含有的“可视型”成分转换为对应的“非可视型”成分，得到完全针对该数据表的“非可视型”问题</li><li>将转译后的问题和阶段一生成的数据表输入Sempre中，得到答案</li></ol><h5 id="阶段三：解释生成-Explanation-generation"><a href="#阶段三：解释生成-Explanation-generation" class="headerlink" title="阶段三：解释生成 (Explanation generation)"></a>阶段三：解释生成 (Explanation generation)</h5><ol><li>借助阶段二中Sempre生成答案过程中使用的λ表达式，利用设计好的一系列规则（模板），将其转化为“可视型”自然语言解释</li><li>输出答案与解释</li></ol><blockquote><p>示例数据流：</p><p><img src="./fig/example_data_stream.png" alt="Example Data Stream"></p></blockquote><h3 id="3-2-技术实现"><a href="#3-2-技术实现" class="headerlink" title="3.2 技术实现"></a>3.2 技术实现</h3><h4 id="3-2-1-阶段一：图表解码-→-信息提取-→-表格重构"><a href="#3-2-1-阶段一：图表解码-→-信息提取-→-表格重构" class="headerlink" title="3.2.1 阶段一：图表解码 → 信息提取 → 表格重构"></a>3.2.1 阶段一：图表解码 → 信息提取 → 表格重构</h4><p>这一阶段主要对输入图表(chart)进行处理，因为 Sempre 接受的输入是关系型数据表和自然语言问题，所以要想使用 Sempre 作为系统核心，我们需要先将输入的图表转化为 Sempre 所要求的数据表格(table)。第一阶段的过程大致可分为三个部分：先将输入图表由可视图形解码为 Vega-Lite 格式代码，然后从代码中提取编码方式和编码数据，最后利用提取的信息构建要输入给 Sempre 的关系型数据表。<br><img src="./fig/stage1_general.png" alt="Stage 1 general"></p><h5 id="图表解码"><a href="#图表解码" class="headerlink" title="图表解码"></a>图表解码</h5><p>解码的目标是得到输入图表的 Vega-Lite 格式源信息，便于之后提取数据和编码方式，具体的解码过程如下：</p><ul><li>若原图表使用 Vega-Lite 格式，那么直接使用</li><li>若原图表使用 D3.js 创建，那么使用 ==D3 deconstructor== 转换为 Vega-Lite 格式</li><li>若原图表使用 bitmap 表示，那么先使用 ==ReVision== 提取数据(data)和标志(mark)，然后手动添加可视编码将其完全转换为 Vega-Lite 格式</li></ul><p>示例图的 Vega-Lite 格式如下：<br><img src="./fig/vega_lite_format.png" alt="Stage 1 general"></p><h5 id="信息提取"><a href="#信息提取" class="headerlink" title="信息提取"></a>信息提取</h5><p>在对图表解码得到 Vega-Lite 格式数据后，从中分别提取可视编码方式和编码数据。可视编码方式就是将数据域映射到可视属性的方式。</p><ul><li>提取编码：数据域 → 可视属性<ul><li>方法：直接在 Vega-Lite 格式中<strong>查找关键字</strong></li><li>例如：<ul><li>柱状图的y方向（可视属性）编码了名词属性的 “Religion”（数据域）</li><li>柱状图的x方向（可视属性）编码了量词属性的 “Percentage”（数据域）</li><li>柱状图的颜色（可视属性）编码了名词属性的 “Response”（数据域）<ul><li>其中，颜色#EE8426 代表 “Common”，#5376A7代表 “Not Common”</li></ul></li></ul></li></ul></li><li>提取数据<ul><li>方法：运行== <strong>Vega-Lite 编译器</strong>==，在数据经过所有转换操作被渲染成图之前<strong>截取，得到全体数据元组</strong>。</li><li>示例 Vega-Lite 格式编译中得到的数据元组（表格）如下图左侧。</li><li>这里的<strong>数据元组</strong>是一种最“简陋”的表格，是所有最小数据单元的简单排列，没有形成行列数据域的“交叉”。在考察数据元组中的任意一个数据项时只需进行一维线性读取。如示例中认为宗教极端主义在 Muslims 中 Common 的比例为 57%，认为宗教极端主义在 Muslims 中 Not common 的比例为 43%，两次读取分两次横向进行；</li><li>而我们日常生活中所使用的表格（如下图右侧）为了发挥表格的二维优势，更加直观、更加方便阅读和查找通常会制造行列数据域的”交叉“，在考察任意一个数据项时需要从横纵两个维度的数据域进行查找对应。例如根据示例数据元组重构的表格（下图右侧）将数据域 “Religion” 和 “Response” 作为正交维度，在考察宗教极端主义在 Muslims 中 Common 和 Not common 时需要从横纵两个方向查找。</li></ul></li></ul><p><img src="./fig/table_reconstruct.png" alt="Table Reconstruct"></p><h5 id="表格重构"><a href="#表格重构" class="headerlink" title="表格重构"></a>表格重构</h5><ul><li>重构目标：<ul><li>从 Vega-Lite 编译器中获取的表格是“平的”（flat），是一个个数据元组的排列，表格中没有形成“交叉”，不是人们通常使用的表格形式，我们需要对它进行重构，使得重构后的表格符合人们通常的使用形式</li></ul></li><li>为什么要重构表格？<ul><li>因为 Sempre 的训练数据集中的表格是人们通常使用的形式，为了使 Sempre 最大程度地发挥作用，给出准确的答案，需要将表格重构为 Sempre 最熟悉最 favor 的样子</li></ul></li><li>怎样重构表格？<ul><li>首先，从原表格中提取枢轴列(pivot column)，将其中的数据项作为新表格的列表头(column header)<ul><li>怎样判断枢轴列？<ul><li>列内容需要以相同的频率重复1次以上</li><li>若有多列满足条件，选重复频率最高的作为枢轴列</li></ul></li></ul></li><li>然后，对数据按照新规定的列表头进行对齐</li><li>例如，上图左侧中原表格 “Religion” 中每个数据项（宗教类型）的重复频率为2，而 “Response” 中每个数据项（Common / Not common）的重复频率为10，选择“Response”作为枢轴列并将其中数据项 “Common / Not common” 作为新表格的列表头，重新对齐数据。</li></ul></li></ul><h4 id="3-2-2-阶段二：图形标志检测-→-语义分析-→-可视属性检测-→-可视操作检测-→-套用编码-→-自然语言转换"><a href="#3-2-2-阶段二：图形标志检测-→-语义分析-→-可视属性检测-→-可视操作检测-→-套用编码-→-自然语言转换" class="headerlink" title="3.2.2 阶段二：图形标志检测 → 语义分析 → 可视属性检测 → 可视操作检测 → 套用编码 → 自然语言转换"></a>3.2.2 阶段二：图形标志检测 → 语义分析 → 可视属性检测 → 可视操作检测 → 套用编码 → 自然语言转换</h4><p>这一阶段主要对输入的自然语言问题（question）进行处理，系统的核心模块 Sempre 接受的输入是关系型数据表和自然语言问题，我们已经在阶段一将输入的图表（chart）转化为了 Sempre 所 favor 的数据表格(table)，但是此时还不能直接将问题同表格一起输入 Sempre，因为在上文中分析过人们对表格的提问习惯：“<em>当用户在分析一个图表时，他们经常会通过引用图表的可视特征（例如柱形与色彩）以及这些特征的可视属性（例如柱宽和色值）提出问题</em>”，将带有可视引用的问题直接输入 Sempre 会导致它不知所云，无法给出正确答案。因此，我们有必要对输入的自然语言问题进行处理，将其中的可视引用转换为对表格中数据域的引用。<br>另外，上文中还分析过<em>人们提问的需要综合多种操作或运算的复合型问题比例为70%</em>，而在包含了可视引用的问题中对需要进行的操作或运算的描述常常是暗示性的，如示例问题中“最长的橙色柱”，我们将“橙色柱”替换为表格中的“回答为普遍的数据”，此时“最长”就显得不合语境了，它实际指的是“比例最高”。因此，我们还需要对问题中此类用语转换为恰当的操作或运算。<br>这一阶段的处理较为复杂，可分为图形标志检测、语义分析、可视属性检测、可视操作检测、套用编码和自然语言转换这六步，其中的主要方法涉及到使用下图中手动创建的 <strong>词表</strong> 进行查找和替换，下面对每步分别详细介绍：</p><p><img src="./fig/word_list.png" alt="Word List"></p><h5 id="图形标志检测"><a href="#图形标志检测" class="headerlink" title="图形标志检测"></a>图形标志检测</h5><ul><li>目标：找到问题中所有指向图表中<strong>图形标志（visual marks）</strong>的用语，即问题所针对的的对象</li><li>方法：从手动创建的<strong>词表</strong>的标志部分中查找匹配</li><li>例如：<ul><li>上图词表中第一部分，对于柱状图的图形标志，提问者可能用会使用“柱”、“长方形”、“部分”等词语来代指柱状图中的一根或多根柱</li><li>对于问题”Which religion has the longest orange component?”，图形标志即为“component”，针对它，问题给出了限定“the longest orange”</li></ul></li></ul><h5 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h5><ul><li>目标：识别问题中对所有图形标志的<strong>描述性用语</strong>，即问题中修饰图形标志的定语</li><li>方法：<ul><li>使用 ==<strong>Stanford CoreNLP dependency parser</strong>== 获取一棵句法分析树</li><li>对树进行以下操作：<ul><li>在树中找到标志词节点</li><li>以该点为源点，向外做BFS</li><li>对遍历到的节点选择性记录（记录有实际意义的成分），被记录下的词即为描述性用语</li></ul></li><li>例如，示例问题的语法分析树如下：</li><li><img src="./fig/dependency_tree.png" alt="Dependency Tree"></li><li>不记录疑问词“which”和冠词“the”，其他有实际意义的词被纳入描述性用语，之间的依赖关系由树边描述</li></ul></li></ul><h5 id="可视属性检测"><a href="#可视属性检测" class="headerlink" title="可视属性检测"></a>可视属性检测</h5><ul><li>目标：从上一步识别出的<strong>描述性用语</strong>中识别出<strong>可视属性（visual attributes）</strong></li><li>方法：<ul><li>从手动创建的<strong>词表</strong>的可视属性部分过滤出<em>阶段一</em>中提取到的可视编码中的可视属性，从而缩小了范围</li><li>然后从<em>描述性用语</em>中逐个词地与过滤后的可视属性词表进行比较、尝试匹配<ul><li>采用基于 ==<strong>Word2vec</strong>== 的近义查询</li><li>遍历所有<em>描述性用语</em>与过滤后的可视属性词，计算它们的相似度，相似度超过一既定阈值的即可匹配</li></ul></li><li>对颜色描述词的特殊处理：<ul><li>从X11颜色表中获取描述词所代指颜色的16进制码</li><li>检查从图表中提取到的所有颜色码</li><li>用与描述词颜色码距离最近的颜色码替换之</li><li>例如：对示例问题中的颜色描述词“orange”的处理</li><li><img src="./fig/color_match.png" alt="Color Match"></li></ul></li></ul></li></ul><h5 id="可视操作检测"><a href="#可视操作检测" class="headerlink" title="可视操作检测"></a>可视操作检测</h5><ul><li>目标：识别描述用语中所暗示的操作或运算</li><li>方法：<ul><li>利用手动创建的<strong>词表</strong>的可视操作（visual operation）部分</li><li>注意：一种可视操作（运算）必针对一个可视属性，如：对属性“长度”求“最大值”，因此，词表中的可视操作部分实际上将每个描述用语都对应到一个（操作，属性）对</li><li>匹配可视操作与描述性用语：与上一步类似地使用基于 ==<strong>Word2vec</strong>== 的近义查询</li><li>例如：示例问题中出现“longest”，便可由词表对应到（argmax，width）</li></ul></li></ul><h5 id="套用编码"><a href="#套用编码" class="headerlink" title="套用编码"></a>套用编码</h5><ul><li>目标和方法：套用阶段一中提取的编码信息，将所有与可视属性、可视操作相关的用语替换为数据域(data field)和数据值(data value)</li><li>例如：可视属性“width”替换为数据域“percentage”，可视属性”color“的取值”#EE8423“替换为数据值”common“</li></ul><h5 id="自然语言转换"><a href="#自然语言转换" class="headerlink" title="自然语言转换"></a>自然语言转换</h5><ul><li>目标：将问题转换为不含任何“可视信息”的，完全是自然语言的问题</li><li>方法：<ul><li>在数据值后补充对应的数据域，例如上一步中替换的数据值“common”后面补充对应的数据域“response”</li><li>重写图形标志词、可视操作和可视属性，如示例问题中标志词“component”重写为“data”、操作“argmax”重写为“most”</li></ul></li></ul><h5 id="处理流程总览"><a href="#处理流程总览" class="headerlink" title="处理流程总览"></a>处理流程总览</h5><p><img src="./fig/stage2_general.png" alt="Stage 2 general"><br>此时，将处理好的完全”非可视“的自然语言问题连同阶段一中得到的关系型数据表输入 Sempre 中，就可以得到问题的回答了。</p><h4 id="3-2-3-阶段三：λ表达式-自然语言转换-→-隐藏域恢复-→-冗余清理-→-语句完善-→-套用编码"><a href="#3-2-3-阶段三：λ表达式-自然语言转换-→-隐藏域恢复-→-冗余清理-→-语句完善-→-套用编码" class="headerlink" title="3.2.3 阶段三：λ表达式-自然语言转换 → 隐藏域恢复 → 冗余清理 → 语句完善 → 套用编码"></a>3.2.3 阶段三：λ表达式-自然语言转换 → 隐藏域恢复 → 冗余清理 → 语句完善 → 套用编码</h4><p>通过前两个阶段，我们从图表中提取信息并构建了关系型数表，将自然语言问题中的可视引用全部转换为针对表格数据域和操作（运算）的非可视引用，并输入 Sempre 得到了答案。在第三阶段，我们要着手生成问题求解过程的自然语言解释（explanation）。这里利用了 Sempre 求解答案过程中的中间产物：输入问题的λ表达式。因为这一表达式本身就是对问题解决方案的描述，我们便充分利用它，设计一系列规则将λ表达式转换为自然语言，并通过规定解释模板将转换后的自然语言进行规范化。最后，因为在解释环节使用可视引用会更加直观，便于用户与原图表对应，所以我们将阶段一中提取的编码信息套用到解释中，将非可视的解释“可视化”，得到最终的解释。</p><h5 id="λ表达式-自然语言转换"><a href="#λ表达式-自然语言转换" class="headerlink" title="λ表达式-自然语言转换"></a>λ表达式-自然语言转换</h5><ul><li><p>λ表达式从何而来？</p><ul><li>Sempre在处理输入的问题时，会将问题转换为λ表达式的格式，该表达式描述了在表格中的要进行的操作（运算）</li><li>例如，示例问题第二阶段转换后的问题“which religion has the most percentage common response data?”和第一阶段构造的表格（见阶段一图2）输入 Sempre 后产生的 λ表达式为：<strong><em>argmax(R[Religion].Row, R[λx(R[Number].R[Common].Religion.x)])</em></strong></li></ul></li><li><p>λ表达式的特点</p><ul><li>λ表达式的运算操作和生成规则都是有限的</li><li>利用这一特点，可以编制有限的规则，将λ表达式转换为类自然语言</li></ul></li><li>编制转换规则<ul><li><img src="./fig/June_2020/lambdaExp2NL_rules.png" alt="lambda expresion to natrual language rules"></li><li>利用以上规则，上文中 λ表达式被转化为：<strong><strong>‘Religion’ of data with the greatest ‘Common’ of ‘Religion’</strong></strong></li></ul></li></ul><h5 id="隐藏域恢复"><a href="#隐藏域恢复" class="headerlink" title="隐藏域恢复"></a>隐藏域恢复</h5><ul><li>隐藏域：在表格重构过程中被隐藏了的数据域</li><li>例如：阶段一中将数据元组重构为关系型数据表时，数据域”percentage”就被隐藏了，只留下了其中的数据值</li><li>恢复方法：将隐藏域用<strong>辅助注释(auxiliary annotation)</strong>的方式保留下来，在遇到对该隐藏域的数据值的引用时，为提高解释的透明性(transparency)，补充上隐藏域</li><li>例如：对于被隐藏的数据域”percentage”，我们在列“Common”和“Not Common”的每一个单元都用辅助注释保留下“percentage”，当需要恢复时，我们就将它拿出来，得到：<strong><strong>‘Religion’ of data with the greatest ==‘Percentage’== of ‘Common’ of ‘Religion’</strong></strong></li></ul><h5 id="冗余清理"><a href="#冗余清理" class="headerlink" title="冗余清理"></a>冗余清理</h5><ul><li>清除两类冗余：<ul><li>数据域或数据项的重复，例如上一步中‘Religion’的重复</li><li>对数据域或“data”一词不必要的提及，例如上一步中的“data”一词</li></ul></li><li>方法：使用正则表达式<ul><li><img src="./fig/redundancy_cleanup_regExp.png" alt="Redundancy Clean up Regular Expression"></li><li>利用以上规则，上文中句子被转换为<strong><strong>‘Religion’ with the greatest ‘Percentage’ of ‘Common’</strong></strong></li></ul></li></ul><h5 id="语句完善"><a href="#语句完善" class="headerlink" title="语句完善"></a>语句完善</h5><ul><li>目标：使生成的解释具备一句话所需的完整的语法结构</li><li>方法：<ul><li>加入主语“I”</li><li>谓语动词：<ul><li>若执行简单的“查找”操作，则用“looked up”</li><li>若执行简单的“计数”操作，则用“counted”</li><li>若执行其他复杂操作，则用“computed”</li></ul></li></ul></li><li>例如，上一步中句子加入主语和谓语后变为 <strong><strong>I looked up ‘Religion’ with the greatest ‘Percentage’ of ‘Common’</strong></strong></li></ul><h5 id="套用编码-1"><a href="#套用编码-1" class="headerlink" title="套用编码"></a>套用编码</h5><ul><li>目标：经过前4步的转换，句子已经很自然了，但是我们希望解释是“可视”的，解释的用语尽可能地指向可视特征，方便用户快速从自然语言对应到图表中，故我们在这一步要使“不可视”文字解释转化为“可视”文字解释</li><li>方法：套用阶段一中提取的编码信息。与第二阶段的用法正好相反，第二阶段是将可视编码“反向”使用，把可视引用转换为不可视引用，而这一阶段要将可视编码“正向”使用，把不可视引用转换为可视引用<ol><li>对于数据域被编码成颜色的数据值，我们直接将其转化为颜色词</li><li>对于被编码成其他可视属性的数据域，我们检查其周围的词是否有对此数据域的操作，并利用阶段二中构造的词表将它们替换为可视属性和操作</li><li>添加图形标志词，并对句子略作调整，使其他词都修饰该标志词</li></ol></li><li>例如：<ol><li>将被编码为颜色的数据域“response”的数据值“common”一词转换为颜色词“color”</li><li>对数据域“percentage”一词，同时考虑相邻的词“greatest”，通过可视编码确定对应的可视操作和属性是“argmax”“width”，然后利用词表将这对（操作，属性）转换为描述词“longest”，</li><li>最后添加图形标志词“bar”，对句子略作调整，得到 <strong><strong>I looked up ‘Religion’ of the longest orange bar.</strong></strong></li></ol></li></ul><h5 id="处理流程总览-1"><a href="#处理流程总览-1" class="headerlink" title="处理流程总览"></a>处理流程总览</h5><p><img src="./fig/stage3_general.png" alt="Stage 3 general"><br>至此，问题答案在第二阶段得到，答案解释也已经在第三阶段生成，将它们输出，系统任务完成。</p><h2 id="四-可改进之处"><a href="#四-可改进之处" class="headerlink" title="四. 可改进之处"></a>四. 可改进之处</h2><h3 id="4-1-自然语言解释可视化"><a href="#4-1-自然语言解释可视化" class="headerlink" title="4.1 自然语言解释可视化"></a>4.1 自然语言解释可视化</h3><p>该系统生成的解释并不是真正的 “可视”解释，其解释方式还是自然语言，仅仅是在用语上指向了图表中的 “可视信息”。实际上，自然语言的解释是不够直观的，用户在验证答案时需要自行将自然语言中的“可视信息”对应到图表中，若能将这一对应过程用真正可视化的“图形”解释生成，交互方式会更友好。</p><h3 id="4-2-系统对复杂问题的支持"><a href="#4-2-系统对复杂问题的支持" class="headerlink" title="4.2 系统对复杂问题的支持"></a>4.2 系统对复杂问题的支持</h3><p>以下给出了4个错误答案的例子<br><img src="./fig/wrong_annswers.png" alt="Wrong Annswers"><br>错误的原因主要来自系统对自然语言问题的解析失当，例如：</p><ul><li>左上图中对position的操作错误，本应是排序，却对应成了查找；</li><li>右上图中对increment along the y axis操作错误，本应是坐标求差，却对应成了计数</li><li>右下图的问题虽然系统答对了，但是一读解释就能知道系统对该问题的理解完全错误，答对纯属巧合</li></ul><p>此类错误出现在第二阶段，这个阶段中对图形标志、可视属性、可视操作的检测均依赖于人工制作的词表，而限于词表的规模，必然无法支持太多灵活的设问方式。另外，鉴于系统是围绕 Sempre 而构建的， 所以难以突破Sempre本身对复杂问题的解答能力。这两点我们从论文中给出的统计图中可以清晰看出：</p><ul><li>系统对需要复合操作的问题的答案准确度（37%）远小于仅需要简单的查找操作的问题的答案准确度（84%）</li><li>在“非可视”问题类型中，系统与Sempre的差别并不大（49% VS 43%），说明了系统的求解能力一定程度上受制于Sempre的求解能力</li></ul><p><img src="./fig/accuracy.png" alt="accuracy"><br>解决这个问题，单纯扩充词表肯定是不够的，需要自然语言处理相关的解决方案。</p><h3 id="4-3-系统对多种图表的支持"><a href="#4-3-系统对多种图表的支持" class="headerlink" title="4.3 系统对多种图表的支持"></a>4.3 系统对多种图表的支持</h3><p>论文中给出的例子多为柱状图，仅有一个折线图的例子。按道理讲，沿用系统流水线，创建更多种图表类型的词表，是可以实现对多种图表的支持的，但是拓展后无疑会对系统的灵活度提出更高的要求，对问题和解释的多样性应该具备足够的支持能力。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Human-Computer Interaction </tag>
            
            <tag> Artificial Intelligence </tag>
            
            <tag> Automatic Answering and Explaining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Elaborate Introduction to t-SNE and its Application</title>
      <link href="/2020/04/25/An-Elaborate-Introduction-to-t-SNE-and-its-Application/"/>
      <url>/2020/04/25/An-Elaborate-Introduction-to-t-SNE-and-its-Application/</url>
      
        <content type="html"><![CDATA[<h1 id="t-SNE-算法详解及应用"><a href="#t-SNE-算法详解及应用" class="headerlink" title="t-SNE 算法详解及应用"></a>t-SNE 算法详解及应用</h1><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><h2 id="基本算法：SNE"><a href="#基本算法：SNE" class="headerlink" title="基本算法：SNE"></a>基本算法：SNE</h2><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>在降维过程中保持数据点在原空间中的近邻关系，即高维空间中距离较小的两点投影到低维空间中的距离也应较小，而原空间中距离较大的两点在投影空间中的距离也应较大，从而在低维空间中拟合数据的大致分布结构（特别是聚类结构）。</p><p>直观地理解，两数据点距离越近，也就是该数据点在许多维度上的取值差异越小，说明它们所代表的对象之间的相似度越高。SNE就是将两点间的<strong>欧式距离</strong>转化为一个代表<strong>相似度</strong>的条件概率值，在高维和低维空间中分别进行度量，其中低维空间中最初数据点的位置是随机初始化的，得到两个分布：</p><ul><li>分布1：对输入对象（即原高维数据集中的全体数据点）两两之间的相似度的度量</li><li>分布2：对输出对象（即投影到低维空间中的全体数据点）两两之间相似度的度量</li></ul><p>进而采用“优化”的方法，以<strong>最小化两个分布的差异</strong>为目标，不断迭代。</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><h4 id="度量高维空间中数据点间相似度"><a href="#度量高维空间中数据点间相似度" class="headerlink" title="度量高维空间中数据点间相似度"></a>度量高维空间中数据点间相似度</h4><p>那么如何将数据点间的“距离”在近邻意义下转换为“相似度”呢？怎样使远距离对应低相似度，近距离对应高相似度呢？SNE借用了正态分布的概念来解决这一问题。正态分布是自然界普遍存在的一种概率分布，如果一个事物受到多个变量的影响，不管每个变量本身符合什么分布，它们叠加后，结果的平均值就是正态分布。</p><blockquote><p>正态分布的奇妙之处，就是许多看似随机的事件竟然服从一个公式就能表达的分布，如同上帝之手特意为之一般。</p></blockquote><p><img src="./pic/gaussian_distribution.png" alt="图1"></p><p>显然，大型高维数据集在空间中的分布大多可以看作是正态分布，准确地说，任取一个数据点作为“中心”，其他全体数据点到该点的距离应满足正态分布。而正态分布的概率密度恰好符合“到中心点（平均值）的距离越近，概率密度越大”的直觉，SNE就是以正态分布的密度函数为原型，逐步完成对两点间“相似度”的定义。</p><p>正态分布密度函数（其中，$\mu$为正态分布的期望，$\sigma$为正态分布的标准差）：</p><script type="math/tex; mode=display">f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)</script><p>首先，对 $(x-\mu)^{2}/2 \sigma^{2}$ 这一项进行改造，原 $(x-\mu)^{2}$ 表示数据值到期望值的距离平方，按照我们的建模意义改造为 $|x<em>{j}-x</em>{i}|^{2}$，表示任意一点$x_j$到选定的中心点$x_i$的距离平方。相应地，原标准差$\sigma$应改造为以$x_i$为中心的数据点间距离的正态分布的标准差$\sigma_i$（这一项的计算会在下文中详细说明）。得到以下公式：</p><script type="math/tex; mode=display">d_{j|i}^{2}=\frac{\|x_{j}-x_{i}\|^{2}}{2\sigma_{i}^{2}}</script><p>根据其意义，可以理解为任意两点之间的“不相似度”。显然，两点之间的距离越远，不相似度就越高。</p><p>这里我们先对标准差$\sigma_i$有一个大致的理解，其详细的计算过程将在后文讨论。对于数据集中的每一个点，以其为中心的正态分布应该是不同的，因为数据密度在不同位置上是不同的：</p><ul><li>若中心点位于较密集区域（即中心点所在簇内数据点间距较小），为了区别其他簇内数据点，使它们的相似度与中心点所在簇内数据点的相似度产生较大差距，正态分布的标准差$\sigma_i$应当较小，使其密度曲线较“窄”较“高”（如图上半部分）。</li><li>若中心点位于较稀疏区域（即中心点所在簇内数据点间距较大），为了保证如此大间距的簇内数据点仍能共同享有较高的相似度，正态分布的标准差$\sigma_i$应当较大，使其密度曲线较“宽”较“矮”（如图下半部分）。</li></ul><p><img src="./pic/dense_vs_sparse.png" alt="图2"></p><p>也就是说计算两点相似度公式中的$\sigma_i$应当视取中心点的不同而不同，若中心点取在数据密度较大的位置处，那么$\sigma_i$应当较小，而若中心点取在数据密度较小的位置处，那么$\sigma_i$应当较大。也正因此，确定不同中心点的$\sigma_i$十分重要，计算方法需要仔细思量。</p><p>接下来，我们回到对任意两点间“相似度”的定义式的设计中。我们改造了原正态分布密度函数中最关键的一部分，并视其为对“不相似度”的度量，那么将改造完成的“不相似度”代入原正态分布密度函数替换原项，得到如下公式，就可以得到我们所期望的“相似度”的定义了吗？</p><script type="math/tex; mode=display">s_{j|i}=\frac{1}{\sqrt{2 \pi} \sigma_i} \exp \left(-\frac{\|x_{j}-x_{i}\|^{2}}{2\sigma_{i}^{2}}\right)</script><p>仔细分析，掐指一算，事情并没有那么简单。两个重要问题浮出水面：</p><ol><li>在计算数据集中任意两点间相似度时，选择哪一个点作为中心点呢？根据我们上文中对$\sigma_i$意义的分析，单纯考虑正态分布的话，似乎选择任何点作为中心点都会更倾向于只是将中心点所在簇与其他簇中的点分开。实际上，采用上式计算得到的结果，是<strong>数据集中任意点与中心点之间的相似度</strong>，中心点所在簇中点的相似度会普遍较高，而其他簇中点的相似度普遍较低。也就是说，仅选取一个中心点用上式计算相似度只会从若干个簇中有效分出一簇（即中心点所在簇），其他簇中点因为相似度相近，仍然混在一起，无法有效区分。</li><li>观察不同标准差的正态分布密度函数图像，不难发现，函数最值的差异会导致密集簇和稀疏簇中两点间的相似度差异。以下图为例：蓝色密集簇中点的相似度可达0.3, 0.35；而橙色稀疏簇中点的相似度只有0.12, 0.15。而这并不是我们所希望发生的情况，因为这样将使得划分簇时没有统一的标准。依然以下图为例：若指定两点间相似度大于0.2的点划归为一簇，那么橙色稀疏簇将无法被划为一簇，相反会被拆解并入其他若干相邻簇中；若指定两点间相似度大于0.1的点划归为一簇，那么蓝色密集簇中将被划入其他簇中的点。理想的结果应该是尽管不同密度的簇选取的标准差$\sigma_i$不同，但一簇内点的相似度应该基本相等。<br><img src="./pic/need_of_scaling.png" alt=""></li></ol><p>如何解决这两个问题呢？既然选取一个中心点计算与其他点间相似度，结果为该中心点所在簇中各点的相似度明显高于其他点，也就是能够有效分出该中心点所在的一个簇；那么，我们就可以将数据集中所有点轮流作为中心点,这样一来，每选取一个中心点进行计算便可分出其所在的某个簇，当遍历完数据集中全体数据点后综合分析，就能得到完整的分簇情况。<br>而第二个问题的解决方案在机器学习领域已经相当成熟，称为“<strong>归一化（Normalization）</strong>”，是特征缩放（Feature Scaling）的方法之一。归一化适合对多个实例进行综合对比评价，它将各实例的各项指标的数值分别除以其总和，得到各实例的各项指标的比例水平，从而避免各项指标的具体数值大小对实例间比较的影响，使得不同量纲的的数据具备可比性。现实世界中有不少运用“归一化”的例子，比如在对比不同国家的经济结构或人口结构时，三大产业的具体产值或农村与城市人口的具体数量是不具有可比性的，因为不同国家的经济总量和人口总数是不同的，科学的对比方法应当是计算各产业占国民经济的比重，农村与城市人口占全国总人口的比例。</p><p>综上所述，我们采取<strong>轮流选取所有点作为中心点，分别计算其余点与中心点的（未缩放）相似度，并进行归一化</strong>的策略（用如下公式表达），算得缩放后的点间相似度$p_{j|i}$，以此度量数据在空间中的分布特点。</p><script type="math/tex; mode=display">p_{j|i}=\frac{\exp(-\|x_{i}-x_{j}\|^{2}/2\sigma_{i}^{2})}{\sum_{k \ne i}\exp(-\|x_{i}-x_{k}\|^{2}/2\sigma_{i}^{2})}</script><p>$p<em>{j|i}$可以理解为：$x_i$选$x_j$为邻居的可能性。需要明确：邻居是根据以点$x_i$为中心的正态分布的概率密度，按比例选取的。（在以点$x_i$为中心的正态分布下，距离$x_i$越近的点，概率密度越大，被选择为邻居的可能性就越高。）由于我们只关心两点之间的相似度，且从模型意义上讲，一个点选择自己做自己的近邻这一行为并无道理，所以定义$p</em>{i|i}=0$.</p><p><img src="./pic/similarity_matrix.png" alt=""></p><blockquote><p>随机变量x的熵是对随机变量不确定性的度量，是对所有可能发生的事件产生的信息量的期望。<br>随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。当随机分布为均匀分布时，熵最大。</p></blockquote><p>每一个$\sigma_i$都会对应一个分布$P_i$，$\sigma_i$越大，该分布的熵就越大。SNE在用户选定参数$perplexity$（混乱度）的基础上，使用二分搜索算法寻找$\sigma_i$。</p><h4 id="度量低维空间中数据点间相似度"><a href="#度量低维空间中数据点间相似度" class="headerlink" title="度量低维空间中数据点间相似度"></a>度量低维空间中数据点间相似度</h4><p>对于数据集中的点在低维空间中的投影，</p><h4 id="在低维空间中拟合高维数据分布"><a href="#在低维空间中拟合高维数据分布" class="headerlink" title="在低维空间中拟合高维数据分布"></a>在低维空间中拟合高维数据分布</h4><h2 id="对SNE的改进：t-SNE"><a href="#对SNE的改进：t-SNE" class="headerlink" title="对SNE的改进：t-SNE"></a>对SNE的改进：t-SNE</h2><h3 id="对称化"><a href="#对称化" class="headerlink" title="对称化"></a>对称化</h3><h3 id="拥挤问题"><a href="#拥挤问题" class="headerlink" title="拥挤问题"></a>拥挤问题</h3><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h4 id="t-分布"><a href="#t-分布" class="headerlink" title="t-分布"></a>t-分布</h4><h4 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h4><h2 id="随机近邻嵌入的特点"><a href="#随机近邻嵌入的特点" class="headerlink" title="随机近邻嵌入的特点"></a>随机近邻嵌入的特点</h2><h4 id="非线性"><a href="#非线性" class="headerlink" title="非线性"></a>非线性</h4><h4 id="无监督"><a href="#无监督" class="headerlink" title="无监督"></a>无监督</h4><h4 id="随机性"><a href="#随机性" class="headerlink" title="随机性"></a>随机性</h4><h2 id="算法改进：应用树结构提升效率"><a href="#算法改进：应用树结构提升效率" class="headerlink" title="算法改进：应用树结构提升效率"></a>算法改进：应用树结构提升效率</h2><h2 id="算法应用"><a href="#算法应用" class="headerlink" title="算法应用"></a>算法应用</h2><h3 id="大型高维数据集的可视化"><a href="#大型高维数据集的可视化" class="headerlink" title="大型高维数据集的可视化"></a>大型高维数据集的可视化</h3><h3 id="机器学习过程解释"><a href="#机器学习过程解释" class="headerlink" title="机器学习过程解释"></a>机器学习过程解释</h3>]]></content>
      
      
      
        <tags>
            
            <tag> t-SNE </tag>
            
            <tag> dimensionality reduction </tag>
            
            <tag> projection &amp; embedding </tag>
            
            <tag> manifold learning </tag>
            
            <tag> visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Estimating Graphlet Statistics via Random Walk</title>
      <link href="/2020/03/19/Estimating-Graphlet-Statistics-via-Random-Walk/"/>
      <url>/2020/03/19/Estimating-Graphlet-Statistics-via-Random-Walk/</url>
      
        <content type="html"><![CDATA[<h1 id="A-General-Framework-for-Estimating-Graphlet-Statistics-via-Random-Walk"><a href="#A-General-Framework-for-Estimating-Graphlet-Statistics-via-Random-Walk" class="headerlink" title="A General Framework for Estimating Graphlet Statistics via Random Walk"></a>A General Framework for Estimating Graphlet Statistics via Random Walk</h1><h1 id="一个通过【随机游走】估算【图微元】相关数据的总体框架"><a href="#一个通过【随机游走】估算【图微元】相关数据的总体框架" class="headerlink" title="一个通过【随机游走】估算【图微元】相关数据的总体框架"></a>一个通过【随机游走】估算【图微元】相关数据的总体框架</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>图微元（Graphlet）是图（Graph）的一种非同构诱导子图，通常由$k$个节点构成，其中k∈{3,4,5}. 由于在识别图的局部拓扑结构方面所表现的优质效果，图微元已被应用到多个领域的分析研究中，例如，在线社交网络（Online Social Networks, OSNs）和生物网络中. 但是，对图微元的搜索和计算十分具有挑战性，原因有二. 首先，现实世界中规模巨大的图结构使得对图微元的计算代价极其昂贵；其次，图的拓扑结构并不容易获得，经常需要使用一些应用程序接口（API）从网络上爬取.</p><blockquote><p>注：图微元是作者根据命名单词词根词缀，结合个人理解翻译而来，若有更好翻译版本，烦请赐教。<br>附：名词后缀-let来源于中古法语或拉丁语，大多用于物件名称，表示“小”，例如：booklet——小册子，leaflet——小叶子；有些是科学名词；少部分加在人称名词上，有轻蔑意味。它的引申意义有“微小，在…佩带的小饰物”。</p></blockquote><h2 id="TO-BE-CONTINUED-…-…"><a href="#TO-BE-CONTINUED-…-…" class="headerlink" title="TO BE CONTINUED … …"></a>TO BE CONTINUED … …</h2>]]></content>
      
      
      
        <tags>
            
            <tag> random walk </tag>
            
            <tag> graphlet </tag>
            
            <tag> Markov chain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Network Tutorial</title>
      <link href="/2020/02/18/Neural-Network-Tutorial/"/>
      <url>/2020/02/18/Neural-Network-Tutorial/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络（Neural-Network）"><a href="#神经网络（Neural-Network）" class="headerlink" title="神经网络（Neural Network）"></a>神经网络（Neural Network）</h1><p>本文为提纲式笔记，对当前最热门的机器学习算法 —— 深度学习（Deep Learning, DL）的基础，神经网络，进行解析。主要介绍神经网络的数学模型与反向传播算法，重点对反向传播算法的原理、过程和实质进行探究，内容包括：神经网络模型表示、应用举例、反向传播算法的流程与实现。</p><blockquote><p>人工神经网络（Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。</p><p>神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种 <strong>非线性统计性数据建模工具</strong>，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。</p></blockquote><p>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p><h1 id="一-模型表示（Model-Representation）"><a href="#一-模型表示（Model-Representation）" class="headerlink" title="一. 模型表示（Model Representation）"></a>一. 模型表示（Model Representation）</h1><h2 id="1-1-数学模型"><a href="#1-1-数学模型" class="headerlink" title="1.1 数学模型"></a>1.1 数学模型</h2><ul><li>简单来讲，神经元（neurons）就是计算单元，它将输入模块（树突）获取电信号输入（称为“spikes”），并将其引导到输出模块（轴突）</li><li>在我们的模型中：<ul><li>轴突就是输入的特征 x_1, x_2, … , x_n，树突输出的就是假设函数的计算结果</li><li>输入单元 x_0 被称作“<strong>偏差单元</strong>”（bias unit），其值总是“1”</li><li>仍使用在逻辑回归中使用过的 S型(sigmoid)函数 作为假设函数，在神经网络中，通常称为“S型<strong>激活函数</strong>”（sigmoid activation function）</li><li>参数θ通常也被称为“<strong>权重</strong>”</li></ul></li><li>节点与层<ul><li>所有输入节点构成神经网络的第一层，通常称为“<strong>输入层</strong>”（Input layer）</li><li>最终输出假设函数结果的节点构成神经网络的最后一层，通常称为“<strong>输出层</strong>”（output layer）</li><li>输入层与输出层之间的层叫做“<strong>隐藏层</strong>”（hidden layers）</li></ul></li><li>举个例子：<ul><li>下面这张图中，我们将隐藏层中的节点记作a，上标代表该节点处于第几层，下标代表该节点是该层的第几个，并将其称为“<strong>激活单元</strong>”（activation units）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_example.png" width = 25% /></div></li><li>其中，各激活单元进行的是如下计算：（其中 <strong><em>Θ</em></strong> 表示相关的参数矩阵，上标表示该矩阵用于第几层的计算，下标表示矩阵的第几行第几列）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_computation.png" width = 55% /></div></li><li>在本例中，我们用一个 3（下一层中除偏差单元以外的激活单元数目）× 4（该层中包含偏差单元的所有单元数目） 的参数矩阵来计算激活节点，将每一行的参数应用到输入中，以获得一个激活节点的值</li><li>假设函数的输出值取决于 1) 上一层激活节点值之和 与 2) 激活函数</li></ul></li></ul><h2 id="1-2-向量化（Vectorization）"><a href="#1-2-向量化（Vectorization）" class="headerlink" title="1.2 向量化（Vectorization）"></a>1.2 向量化（Vectorization）</h2><ul><li>下面，我们将继续用上面所举的例子，对计算式进行向量化：<ul><li>新定义向量 <strong><em>z</em></strong> ，上标代表节点所在层，下标表示节点在该层中的序号<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_example_replace.png" width = 14% /></div></li><li>也就是说，向量 <strong><em>z</em></strong> 意味着：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vector_z.png" width = 40% /></div></li><li>当向量 <strong><em>x</em></strong> 是隐藏（激活）层 <strong><em>a</em></strong> 时，可将上式写为通式：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vector_z_rewrite.png" width = 20% /></div></li><li>最后，输出层：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/neural_network_hypothesis_vectorized.png" width = 27% /></div></li></ul></li></ul><h2 id="1-3-应用举例（Applications）"><a href="#1-3-应用举例（Applications）" class="headerlink" title="1.3 应用举例（Applications）"></a>1.3 应用举例（Applications）</h2><h3 id="用简单神经网络模拟逻辑门"><a href="#用简单神经网络模拟逻辑门" class="headerlink" title="用简单神经网络模拟逻辑门"></a>用简单神经网络模拟逻辑门</h3><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/simple_nn_individual_example.png" width = 90% /></div><h3 id="用多层神经网络模拟复杂逻辑进行分类"><a href="#用多层神经网络模拟复杂逻辑进行分类" class="headerlink" title="用多层神经网络模拟复杂逻辑进行分类"></a>用多层神经网络模拟复杂逻辑进行分类</h3><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/simple_nn_mixed_example.png" width = 90% /></div><h2 id="1-4-多分类任务"><a href="#1-4-多分类任务" class="headerlink" title="1.4 多分类任务"></a>1.4 多分类任务</h2><p>在多分类任务中，我们有多个输出单元，最终输出的向量长度等于类别数量，且向量中只有一个元素为“1”，代表预测的类别，其他元素为“0”</p><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_multiple_output_oneVSall.png" width = 90% /></div><h1 id="二-代价函数与反向传播"><a href="#二-代价函数与反向传播" class="headerlink" title="二. 代价函数与反向传播"></a>二. 代价函数与反向传播</h1><h2 id="2-1-代价函数（Cost-Function）"><a href="#2-1-代价函数（Cost-Function）" class="headerlink" title="2.1 代价函数（Cost Function）"></a>2.1 代价函数（Cost Function）</h2><ul><li><p>规定以下记号：</p><ul><li><strong><em>L</em></strong>: 网络的总层数</li><li><strong><em>s_l</em></strong>：第 <em>l</em> 层中的节点数量（除去偏差单元）</li><li><strong><em>K</em></strong>：输出层的节点数（即类的数量）</li><li>因为神经网络可能有很多输出单元，我们记<strong><em>h(x)_k</em></strong>为假设函数结果的第k个值</li></ul></li><li><p>公式：</p><ul><li>基于逻辑回归的代价函数<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_logisticReg_costFunction.png" width=80%></div></li><li>改造为神经网络的代价函数<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_cost_function.png" width=85%></div></li></ul></li><li>对比：<ol><li>参数量的增加，由一个假设函数的参数向量<strong><em>θ</em></strong> ，到L-1层神经网络的参数矩阵<strong><em>Θ</em></strong> </li><li>公式的前半部分由一重累加和变为二重累加和，因为要将输出层中的K个单元的计算结果相加</li><li>公式的后半部分由一重累加和变为三重累加和，因为要将L-1层神经网络每层的参数矩阵的所有元素平方相加</li><li>注意：类似于逻辑回归，对常数项θ_0，我们想象为θ_0 * x_0，并且不对其进行正则化；在神经网络中对于每层参数矩阵的第0行第0列不进行正则化</li></ol></li></ul><h2 id="2-2-反向传播算法（Backpropagation）"><a href="#2-2-反向传播算法（Backpropagation）" class="headerlink" title="2.2 反向传播算法（Backpropagation）"></a>2.2 反向传播算法（Backpropagation）</h2><ul><li>注意：反向传播算法并非一个不同于梯度下降的算法，而是梯度下降算法运行在神经网络中时比较有效的计算方法</li><li>反向传播算法（Backpropagation）包括前向传播（Forward pass）和后向传播（Backward pass）两个过程，分别承担不同的计算任务</li><li>链式法则（chain rule）—— 反向传播算法的数学基础  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/chain_rule.png" width=45%></div></li><li>例子：（理解反向传播算法，李宏毅）<ul><li>假设有如下神经网络架构：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_nn.png" width=50%></div></li><li>对其输入层与第二层第一单元之间的关系进行分析：<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute.png" width=50%></div><br>计算 <strong>代价函数</strong> <strong><em>C</em></strong> 对 <strong>参数</strong> <strong><em>w</em></strong> 的导数，根据链式法则，可拆解为计算 <strong>函数</strong> <strong><em>z(w)</em></strong> 对 <strong>参数</strong> <strong><em>w</em></strong> 的导数与 <strong>代价函数</strong> <strong><em>C</em></strong> 对 <strong>变量</strong> <strong><em>z</em></strong> 的导数之积。其中，第一部分可以在 <strong>前向传播</strong> 的过程中直接得到：函数 <strong><em>z(w)</em></strong> 对 参数 <strong><em>w</em></strong> 的导数即为前一层网络中与参数w相关的变量 <strong><em>x</em></strong>。但是第二部分则需要在 <strong>后向传播</strong> 中计算</li><li>后向传播计算过程推导<br>考察下一层神经网络（假设所有神经元的激活函数均为sigmoid函数），如图：<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute1.png" width=70%></div><br>由链式法则，代价函数 <strong><em>C</em></strong> 对 变量 <strong><em>z</em></strong> 的导数可拆解为激活函数 <strong><em>a(z)</em></strong> 对 变量 <strong><em>z</em></strong> 的导数与代价函数 <strong><em>C</em></strong> 对 变量 <strong><em>a</em></strong> 的导数。其中，第一部分由激活函数的形式而确定；而第二部分，鉴于此神经元的计算结果（受参数w影响）参与到了下一层两个神经元的计算中，由链式法则，可以得出如图中最下方计算公式，简单整理后，在最右侧公式中，便可以发现递归计算的身影<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute2.png" width=40%></div><br>通过以上推导过程，我们不难看出：类似于递归算法，若想计算代价函数对某一个参数（第1层中某个单元）的导数，必须先计算代价函数对第2层某个单元的导数，进而必须先计算代价函数对第3层某些(≥2)单元的导数，……，如此层层深入，先得出后面结果，否则无从计算前面。这就是 <strong>后向传播</strong> 的计算过程</li><li>后向传播在网络中的“计算流”<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_nn1.png" width=50%></div></li><li>小结：反向传播的两大过程 —— “前向传播”与“后向传播”<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_summary.png" width=50%></div></li></ul></li><li>算法过程<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_algo.png" width=70%></div><ul><li>导入训练数据集，将神经网络每一层的偏差矩阵 <strong><em>∆^(l)</em></strong> 初始化为零矩阵</li><li>对于m个训练数据，依次进行：<ul><li>将特征向量输入第一层网络（输入层）</li><li>执行前向传播，计算并保存沿途各层的输出结果 <strong><em>a^(l), l = 2, 3, … , L</em></strong></li><li>计算网络输出与真实值的偏差向量 <strong><em>δ^(L)</em></strong></li><li>执行后向传播，计算并保存沿途各层的偏差向量 <strong><em>δ^(l), l = L-1, L-2, … , 2</em></strong>，向量化后得到如下式子：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_delta_partial_compute.png" width=30%></div>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_delta_vectorized.png" width=40%></div></li><li>更新各层偏差矩阵 <strong><em>∆^(l)</em></strong>，向量化公式如下：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_deltaCap_compute.png" width=30%></div></li></ul></li><li>计算代价函数对参数矩阵的偏导数<ul><li>矩阵 <strong><em>D^(l)</em></strong> 用作“累加器”，在计算的过程中把各个分部结果累加起来，最终计算出我们所求的偏导数</li></ul></li></ul></li></ul><h2 id="2-3-反向传播算法实践"><a href="#2-3-反向传播算法实践" class="headerlink" title="2.3 反向传播算法实践"></a>2.3 反向传播算法实践</h2><h3 id="实现细节：参数展开（Unrolling-Parameters）"><a href="#实现细节：参数展开（Unrolling-Parameters）" class="headerlink" title="实现细节：参数展开（Unrolling Parameters）"></a>实现细节：参数展开（Unrolling Parameters）</h3><ul><li>不同于线性回归和逻辑回归，当我们使用神经网络时，参数的数学组织形式不再是向量，而是矩阵<ul><li><strong><em> Θ^(1), Θ^(2), Θ^(3), … </em></strong></li><li><strong><em> D^(1), D^(2), D^(3), … </em></strong></li></ul></li><li>为了能够使用 <strong><em>fminunc()</em></strong> 等高级优化算法，我们需要将以上矩阵“展开”成向量。例如，在Octave/MATLAB中使用以下命令：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thetaVector &#x3D; [ Theta1(:); Theta2(:); Theta3(:); ]</span><br><span class="line">deltaVector &#x3D; [ D1(:); D2(:); D3(:) ]</span><br></pre></td></tr></table></figure></li><li>从“展开”后的向量变回矩阵（假设原矩阵Theta1的大小为10x11, Theta2的大小为10x11，Theta3的大小为1x11）：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 &#x3D; reshape(thetaVector(1:110),10,11)</span><br><span class="line">Theta2 &#x3D; reshape(thetaVector(111:220),10,11)</span><br><span class="line">Theta3 &#x3D; reshape(thetaVector(221:231),1,11)</span><br></pre></td></tr></table></figure></li><li>小节：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/unroll_parameters.png" width=60%></div></li></ul><h3 id="实现细节：梯度检查（Gradient-Checking）"><a href="#实现细节：梯度检查（Gradient-Checking）" class="headerlink" title="实现细节：梯度检查（Gradient Checking）"></a>实现细节：梯度检查（Gradient Checking）</h3><ul><li>作用：检查神经网络反向传播算法的正确性 —— 是否正确计算梯度</li><li>由梯度的几何意义（切线），代价函数J(Θ)对特定参数Θ的偏导数可以计算如下：<ul><li>当只有一个参数矩阵时：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/derivative_approximation.png" width=35%></div></li><li>当有多个参数矩阵时，对Θ_j的偏导数：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/derivative_approximation1.png" width=65%></div></li><li>其中，<strong><em>ε(epsilon)</em></strong> 为任意极小值，一般可以取10^(-4)，若太小会出现数值计算问题</li></ul></li><li>Octave/MATLAB实现：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epsilon &#x3D; 1e-4;</span><br><span class="line">for i &#x3D; 1:n,</span><br><span class="line"> thetaPlus &#x3D; theta;</span><br><span class="line"> thetaPlus(i) +&#x3D; epsilon;</span><br><span class="line"> thetaMinus &#x3D; theta;</span><br><span class="line"> thetaMinus(i) -&#x3D; epsilon;</span><br><span class="line"> gradApprox(i) &#x3D; (J(thetaPlus) - J(thetaMinus))&#x2F;(2*epsilon)</span><br><span class="line">end;</span><br></pre></td></tr></table></figure></li><li>一旦我们计算出gradApprox，与之前计算得到的deltaVector进行比较，若 gradApprox ≈ deltaVector 则说明反向传播算法实现正确</li><li>一旦验证了反向传播算法是正确的，就不需要再次计算gradApprox，因为gradApprox的计算代码运行很慢</li></ul><h3 id="实现细节：随机初始化（Random-Initialization）"><a href="#实现细节：随机初始化（Random-Initialization）" class="headerlink" title="实现细节：随机初始化（Random Initialization）"></a>实现细节：随机初始化（Random Initialization）</h3><ul><li>不同于线性回归和逻辑回归中将参数向量初始化为零向量，在神经网络中，参数矩阵不能初始化为零矩阵，因为这会使得隐藏层的各个单元输入均为零</li><li>而对参数矩阵的其他形式的初始化也不一定行得通，例如下图中的初始化方式 —— 来自同一神经元的联结参数相等（在图中使用同种颜色表示），会导致后面所有神经元在反向传播时的计算结果相同，即所有神经元计算同一函数，捕捉同一特征，使得神经网络“空有其表”，形成巨大冗余<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_wrong_init.png" width=50%></div></li><li>因此，需要对神经网络的参数矩阵进行 <strong>随机初始化</strong>：将其赋值为区间[-ε,ε]中的随机数（注：此处的epsilon与梯度检查中的epsilon没有任何关系）</li><li>Octave/MATLAB实现：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span><br><span class="line">% rand(x,y) is just a function in octave that will initialize </span><br><span class="line">% a matrix of random real numbers between 0 and 1.</span><br><span class="line"></span><br><span class="line">Theta1 &#x3D; rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 &#x3D; rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 &#x3D; rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure></li></ul><h3 id="实现小节（Summary）"><a href="#实现小节（Summary）" class="headerlink" title="实现小节（Summary）"></a>实现小节（Summary）</h3><ul><li>首先，选择一个神经网络架构：共有多少层、每层有多少个神经元 ……<ol><li>输入层单元数 = 特征向量的维度</li><li>输出层单元数 = 类别数量</li><li>隐藏层单元数： 通常情况下越多越好，但是需要权衡随着单元数量增加导致的计算成本增加</li><li>如果隐藏层的数量大于1，那么最好每层的神经元数量是相等的</li></ol></li><li>训练神经网络<ol><li>随机初始化参数（权重）</li><li>执行前向传播，对每个特征向量计算其预测值</li><li>计算代价函数</li><li>执行后向传播，计算梯度/偏导数</li><li>使用梯度检查，确保反向传播算法执行正确，然后关闭检查</li><li>使用梯度下降算法或其他高级优化算法最小化代价函数得到参数值</li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> gradient descent </tag>
            
            <tag> neural network </tag>
            
            <tag> backpropagation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression Tutorial</title>
      <link href="/2020/02/14/Logistic-Regression-Tutorial/"/>
      <url>/2020/02/14/Logistic-Regression-Tutorial/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归（Logistic-Regression）"><a href="#逻辑回归（Logistic-Regression）" class="headerlink" title="逻辑回归（Logistic Regression）"></a>逻辑回归（Logistic Regression）</h1><p>本文为提纲式笔记，在解决线性回归问题的基础上探讨逻辑回归，主要介绍逻辑回归的机器学习算法与正则化技术。内容包括：逻辑回归问题的简介、相关基础术语解析、梯度下降算法、其他高级优化算法的使用、多分类问题解法、正则化技术简介及其在线性回归与逻辑回归中的应用。</p><blockquote><p>逻辑回归是一种对数几率统计模型，其基本形式是使用Logistic函数（一种常见的S型函数）来对二元因变量建模，并且存在着许多复杂的扩展。逻辑回归常用于解决二分类问题（在解决多分类问题时可以使用softmax方法进行处理），用于估计某个事件的可能性。</p><p>逻辑回归与线性回归的关系：逻辑回归与线性回归都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 <strong><em>y</em></strong> 服从伯努利分布，而线性回归假设因变量 <strong><em>y</em></strong> 服从高斯分布。 因此与线性回归有很多相同之处，去除Logistic映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Logistic函数引入了非线性因素，因此可以轻松处理二分类问题。</p></blockquote><h1 id="一-分类问题与逻辑回归"><a href="#一-分类问题与逻辑回归" class="headerlink" title="一. 分类问题与逻辑回归"></a>一. 分类问题与逻辑回归</h1><h2 id="1-1-分类（Classification）"><a href="#1-1-分类（Classification）" class="headerlink" title="1.1 分类（Classification）"></a>1.1 分类（Classification）</h2><ul><li>例子：<ul><li>邮件：垃圾邮件/正常邮件</li><li>线上交易：诈骗/正常</li><li>肿瘤：恶性/良性</li></ul></li><li>分类问题就像回归问题一样，只是我们现在要预测的值只包含少量的离散值。</li><li>典型的<strong>二分类</strong>（binary classification problem）任务：y∈{0，1}<ul><li>y=0时一般表示某种性质的缺失（Negative class）</li><li>y=1时一般表示某种性质的存在（Positive class）</li></ul></li><li>分类任务不能用回归方法解决<ul><li>回归方法：计算出线性回归函数后，映射所有大于0.5的预测作为1，所有小于0.5的预测作为0</li><li>回归方法并不适用，因为分类问题实际上不存在一个能够拟合的线性函数</li></ul></li><li>注意：逻辑回归实际上并非用于回归任务，而是用于分类任务</li></ul><h2 id="1-2-假设函数的表达式（Hypothesis-Representation）"><a href="#1-2-假设函数的表达式（Hypothesis-Representation）" class="headerlink" title="1.2 假设函数的表达式（Hypothesis Representation）"></a>1.2 假设函数的表达式（Hypothesis Representation）</h2><ul><li>需要改变假设函数h(x)的形式以将其输出范围限制在(0, 1)区间内</li><li>做法如下：将theta^T*X放入一个特殊的函数中进行映射  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/sigmoid_function.png" width = 20% /></div></li><li>该函数称为<strong>S型函数(Sigmoid Function)</strong> 或<strong>逻辑函数(Logistic Function)</strong>，图像如下：<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/sigmoid_function_image.png" width = 80% /></div><h3 id="假设函数表达的意义"><a href="#假设函数表达的意义" class="headerlink" title="假设函数表达的意义"></a>假设函数表达的意义</h3></li><li>h(x)将给出输出值为“1”的<strong>概率</strong><ul><li>例如：若h(x)=0.7，则输出值为“1”（具有某种性质）的概率为70% ，而输出值为“0”（不具有某种性质）的概率为30%</li></ul></li><li>数学表达即为：<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/classification_hx_meaning.png" width = 40% /></div><br>  其中，P(y=1|x;θ) 表示在给定假设函数的输入值x及其相关参数值θ后，输出值为1的概率</li></ul><h2 id="1-3-决策边界（Decision-Boundary）"><a href="#1-3-决策边界（Decision-Boundary）" class="headerlink" title="1.3 决策边界（Decision Boundary）"></a>1.3 决策边界（Decision Boundary）</h2><ul><li>为了得到离散的0或1分类，我们可以将假设函数（Sigmoid）的输出转换如下：<ul><li>h(x)≥0.5 -&gt; y=1 即 x≥0 -&gt; y=1</li><li>h(x)<0.5 -> y=0 即 x<0 -> y=0</li></ul></li><li>从图像上来看，假设函数会形成一条决策边界，边界一侧的数据点预测为“1”，另一侧预测为“0”  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/decision_boundary_image.png" width = 80% /></div></li></ul><h2 id="1-4-代价函数（Cost-Function）"><a href="#1-4-代价函数（Cost-Function）" class="headerlink" title="1.4 代价函数（Cost Function）"></a>1.4 代价函数（Cost Function）</h2><ul><li>线性回归的代价函数在分类问题中并不适用<ul><li>因为 1）分类问题中假设函数h(x)的形式很复杂； 2）我们对每一次预测产生的代价cost(h(x),y)的定义为预测与真实标签值之差的平方，平方之后无疑使得整个代价函数更为复杂<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/cost_redefine.png" width = 50% /></div></li><li>事实上，若沿用线性回归的“均方误差”代价函数，则会导致代价函数J(θ)“非凸”（non-convex）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/convex_vs_nonconvex.png" width = 80% /></div></li></ul></li><li>定义适用于逻辑回归的预测代价<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/cost_of_classification.png" width = 65% /></div><ul><li>通过观察图像理解函数意义<ul><li>当实际标签值y=1时：<ul><li>若预测值 h(x) -&gt; 0，则代价 cost -&gt; ∞.（预测失误）</li><li>若预测值 h(x) -&gt; 1，则代价 cost -&gt; 0.（预测正确）</li></ul></li><li>当实际标签值y=0时：<ul><li>若预测值 h(x) -&gt; 0，则代价 cost -&gt; 0.（预测正确）</li><li>若预测值 h(x) -&gt; 1，则代价 cost -&gt; ∞.（预测失误）</li></ul></li></ul></li><li><strong><em>可以理解为我们对算法的预测失误进行处罚，失误越大，处罚越严厉</em></strong></li></ul></li><li>将新定义的 预测代价（cost）代入 代价函数（cost function）中，就可以保证 J(θ) 是凸函数了</li></ul><h2 id="1-5-简化代价函数"><a href="#1-5-简化代价函数" class="headerlink" title="1.5 简化代价函数"></a>1.5 简化代价函数</h2><ul><li>将代价cost(h(x), y)由两种情况的分段函数写为统一形式:<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/calssification_cost.png" width = 60% /></div></li><li>将上式代入代价函数J(θ)中，得到完整的代价函数表达式：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/calssification_cost_function.png" width = 65% /></div></li><li>在实际应用中，进行向量化之后的表达式为：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vectorized_cost_function.png" width = 55% /></div></li></ul><h2 id="1-6-梯度下降"><a href="#1-6-梯度下降" class="headerlink" title="1.6 梯度下降"></a>1.6 梯度下降</h2><ul><li>梯度下降的通式为：<div style="text-indent:5%"><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent_general.png" width = 32% /></div></li><li>计算末尾的微分项后得到：<div style="text-indent:5%"><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/classification_gradient_descent.png" width = 45% /></div></li><li>向量化后得到：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vectorized_classification_gradient_descent.png" width =40% /></div></li></ul><h2 id="1-7-高级优化算法"><a href="#1-7-高级优化算法" class="headerlink" title="1.7 高级优化算法"></a>1.7 高级优化算法</h2><ul><li>优化算法：<ul><li>给定代价函数J(θ)，欲求使得J(θ)最小的参数θ</li><li>算法执行过程中，对于当前的θ，需要计算:<ol><li>J(θ)的函数值</li><li>J(θ)对各个θ_j (j = 0,1,…,n)的导数值</li></ol></li><li>在Octave/MATLAB中：可在编程实现计算以上两者后传参至函数fminunc，fmincg等函数接口中调用高级优化算法</li><li>优化算法举例：<ul><li>梯度下降</li><li>Conjugate Gradient</li><li>BFGS</li><li>L-BFGS</li></ul></li><li>高级优化算法不需要手动调试学习率α，并且通常执行较快；但是算法过程很复杂</li></ul></li></ul><h2 id="1-8-多分类任务：一对多算法（Multiclass-Classification-One-vs-all）"><a href="#1-8-多分类任务：一对多算法（Multiclass-Classification-One-vs-all）" class="headerlink" title="1.8 多分类任务：一对多算法（Multiclass Classification: One-vs-all）"></a>1.8 多分类任务：一对多算法（Multiclass Classification: One-vs-all）</h2><ul><li>举例：<ul><li>邮件标签：工作，朋友，家庭，爱好，…</li><li>医疗诊断：无病，感冒，流感，…</li><li>天气：阴，晴，雨，雪，…</li><li>需要将预测类别从 y∈{0, 1} 二类拓展到 y∈{0, 1, … , n} 多类</li></ul></li><li>假设函数的意义：某数据点属于该类的概率，如果将多分类任务划分为n个二分类任务，那么得到的n个假设函数分别就代表某数据点属于某一类的概率，取这n个概率值中最大的一个为该数据点的预测结果</li><li>一对多算法：<ul><li>以三类为例：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_vs_all.png" width = 100% /></div></li><li>数学表达：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiclass_classification.png" width = 40% /></div></li></ul></li></ul><h1 id="二-正则化（Regularization）"><a href="#二-正则化（Regularization）" class="headerlink" title="二. 正则化（Regularization）"></a>二. 正则化（Regularization）</h1><h2 id="2-1-过拟合（Overfit）"><a href="#2-1-过拟合（Overfit）" class="headerlink" title="2.1 过拟合（Overfit）"></a>2.1 过拟合（Overfit）</h2><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/overfit_vs_underfit.png" width = 90% /></div><ul><li><strong>欠拟合</strong>：算法具有“<strong>高偏差 (High bias)</strong>” —— 假设函数h(x)没有较好地拟合数据，该函数的与数据的趋势很不匹配,它通常是由于函数太简单或使用的特征（变量）太少造成的。</li><li><strong>过拟合</strong>：算法具有“ <strong>高方差 (High variance)</strong> ” —— 假设函数h(x)对数据过度拟合，该函数虽然适合现有数据，但不能很好地 <strong>泛化</strong> 以预测新数据，它通常是由于函数太复杂造成的，会产生许多与数据无关的不必要的曲线和角度。</li><li>解决过拟合问题有两个主要选项：<ol><li>减少特征（变量）的数量<ul><li>手动选择保留哪些特征</li><li>使用某种模型选择算法</li></ul></li><li>正则化<ul><li>保留所有特征，但减少参数θ的大小</li><li>当我们有很多具有微小作用的特征时，正则化将表现非常好</li></ul></li></ol></li></ul><h2 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2.2 代价函数"></a>2.2 代价函数</h2><ul><li>如果假设函数h(x)过拟合，我们可以通过增加代价函数来减少函数中某些变量（特征）的权重</li><li>举个例子：我们想使一个四次函数（如下）形状更趋近于二次函数<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/quartic_function.png" width = 45% /></div><ul><li>那么我们想尽可能地弱化 x^3 和 x^4 的影响，在不去除这些特征，也不改变假设函数形式的前提下，我们可以修改代价函数:<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_cost_function_example.png" width = 70% /></div></li><li>通过在代价函数中加上 θ_3 和 θ_4 相关且对代价函数值影响非常大的项，在降低代价的过程中，我们就必须缩小 θ_3 和 θ_4 ，即将 x^3 和 x^4 的权重降低</li></ul></li><li>通常情况下，我们不知道哪几个特征是需要被降权的，所以，我们可以在一个简单的求和中正则化所有的参数:<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_cost_function.png" width = 60% /></div><ul><li>其中，λ 是正则化参数，它决定了参数的代价扩大了多少</li></ul></li><li>利用上述带有额外一求和项的代价函数，我们可以使假设函数的形式更加平滑以减少过拟合</li><li>注意：如果 λ 太大，它可能会使函数过于平滑并导致欠拟合。</li></ul><h2 id="2-3-正则化的线性回归"><a href="#2-3-正则化的线性回归" class="headerlink" title="2.3 正则化的线性回归"></a>2.3 正则化的线性回归</h2><ul><li>代价函数  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_cost_function.png" width = 60% /></div></li><li>梯度下降<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_GD.png" width = 90% /></div><ul><li>这里将 θ_0 与其他参数 θ_j (j = 1, 2, … , n) 分开处理，我们将 θ_j 的式子变形如下：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_GD_transform.png" width = 65% /></div></li><li>上式中等号右边第一项一般都是小于1的，这就起到了在每次迭代中缩小 θ_j 的作用。注意，此时等号右边第二项与正则化之前一样</li></ul></li><li>标准方程<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_NE.png" width = 45% /></div><br>  方程的大致形式与原来的方程是一样的，只不过在括号里增加了另一项<br>  注意：当 m&lt;n 时，(X^T)X 是不可逆的；但是当我们加入 λL 后，(X^T)X+λL 就是可逆的了</li></ul><h2 id="2-4-正则化的逻辑回归"><a href="#2-4-正则化的逻辑回归" class="headerlink" title="2.4 正则化的逻辑回归"></a>2.4 正则化的逻辑回归</h2><ul><li>代价函数  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_logisticReg_costFunction.png" width = 95% /></div></li><li>梯度下降<ul><li>梯度下降过程与线性回归是一致的，只是式中假设函数 h(x) 不同<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_linearReg_GD.png" width = 90% /></div></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> gradient descent </tag>
            
            <tag> logistic regression </tag>
            
            <tag> classification </tag>
            
            <tag> regularization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linear Regression Tutorial</title>
      <link href="/2020/02/07/Linear-Regression-Tutorial/"/>
      <url>/2020/02/07/Linear-Regression-Tutorial/</url>
      
        <content type="html"><![CDATA[<h1 id="线性回归（Linear-Regression）"><a href="#线性回归（Linear-Regression）" class="headerlink" title="线性回归（Linear Regression）"></a>线性回归（Linear Regression）</h1><p>本文为提纲式笔记，以线性回归问题作为机器学习的入门研究对象，主要介绍一元和多元线性回归的机器学习算法，并提及多项式回归问题和标准方程解法。内容包括：线性回归问题的简介、相关基础术语解析、梯度下降算法介绍与应用、标准方程解法对比。</p><blockquote><p>在统计学中，<strong>线性回归</strong> 是利用称为线性回归方程的最小二乘函数（最小化 <strong>误差的平方和</strong> 寻找数据的最佳函数匹配）对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为一元回归或简单回归，大于一个自变量情况的叫做多元回归。（Wikipedia）</p></blockquote><p>线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。</p><h1 id="一-一元线性回归"><a href="#一-一元线性回归" class="headerlink" title="一. 一元线性回归"></a>一. 一元线性回归</h1><h2 id="1-1-一些记号"><a href="#1-1-一些记号" class="headerlink" title="1.1 一些记号"></a>1.1 一些记号</h2><ul><li>m：训练样本规模</li><li>x：输入变量/特征</li><li>y：输出变量/目标变量</li><li>(x,y)：一个训练样本</li><li>h：hypothesis（假设）—— 算法学习到的函数</li></ul><h2 id="1-2-代价函数"><a href="#1-2-代价函数" class="headerlink" title="1.2 代价函数"></a>1.2 代价函数</h2><ul><li>用代价函数来衡量假设函数h的准确性：它取假设函数h对输入x的所有结果h(x)与和实际输出y的平均值之差(实际上是平均值的更fancy的版本)。</li><li><strong>平方误差函数</strong>（squared error function）或<strong>均方误差</strong>（mean squared error）。</li></ul><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/mean_squared_error.png" width = 60% /></div><ul><li>式中取均值后再乘以1/2，是因为它更够使得梯度下降的计算更加方便，因为平方式求导后的系数会将其抵消。</li></ul><h2 id="1-3-梯度下降"><a href="#1-3-梯度下降" class="headerlink" title="1.3 梯度下降"></a>1.3 梯度下降</h2><ul><li>过程类似：下山（二维情况，如下图）。在当前位置环视四周，寻找当前最快的下山路径，不断重复。最终，山的最低点就是代价函数的最小值</li></ul><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent.png" width = 60% /></div><ul><li>做法：对代价函数求导，该点的导数就是山的坡度，也就是移动的方向。代价函数沿着当前最陡的下降方向移动，移动的步长通过参数\alpha —— <strong>学习率</strong>（learning rate）—— 决定。</li><li>学习率决定下降的步长，代价函数的导数决定下降的方向</li><li>不同起始点出发后最后到达的最低点可能不同</li><li>算法：</li></ul><p style="text-indent:30%"> repeat until convergence:</p><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent_algo.png" width = 30% /></div><p style="text-indent:30%"> where j=0,1 represents the feature index number.</p><ul><li>注意：对于每一轮迭代，参数的更新应该是同步的：在计算完毕后再统一更新；在有其他参数未完成计算之前更新某一已完成计算的参数会造成错误</li></ul><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/gradient_descent_implement.png" width = 70% /></div><ul><li>关于学习率（lr）<ul><li>若lr太小，下降速度会很慢</li><li>若lr太大，下降过程中可能会越过最低点，无法收敛，甚至会发散</li><li>梯度下降算法为什么在固定lr的情况下仍然能收敛？<ul><li>在接近最低点的过程中，代价函数的导数项会逐渐变小，使得移动步长逐渐变小</li></ul></li></ul></li><li><strong>批量梯度下降</strong> （Batch Gradient Descent）<ul><li>下降的每一步都用上所有的训练数据（全集）</li><li>反映在一元线性回归中：每次都是计算累加值</li></ul></li></ul><h1 id="二-多元线性回归"><a href="#二-多元线性回归" class="headerlink" title="二. 多元线性回归"></a>二. 多元线性回归</h1><h2 id="2-1-一些记号"><a href="#2-1-一些记号" class="headerlink" title="2.1 一些记号"></a>2.1 一些记号</h2><ul><li>m：训练样本规模</li><li>n：变量/特征/维度数</li><li>x^(i)：第i个训练样本的特征向量</li><li>x^(i)_j：第i个训练样本的特征向量的第j个特征</li><li>h：hypothesis（假设）—— 算法学习到的函数</li></ul><h2 id="2-2-回归函数"><a href="#2-2-回归函数" class="headerlink" title="2.2 回归函数"></a>2.2 回归函数</h2><ul><li><p>多元线性回归拟合（假设）函数形式：</p>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_function.png" width = 60% /></div><ul><li>x_1, x_2, … , x_n 表示n个变量（特征）</li><li>theta_0, theta_1, … , theta_n 为需要计算的参数</li></ul></li><li><p>对任意 i ∈ 1,…,m，x^(i)_0 = 1. 那么将上式向量化后写为：</p><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_function_vector.png" width = 60% /></div></li></ul><h2 id="2-3-代价函数"><a href="#2-3-代价函数" class="headerlink" title="2.3 代价函数"></a>2.3 代价函数</h2><ul><li>与一元线性回归的代价函数形式相同，均采用<strong>平方误差函数</strong>：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/cost_function.png" width = 45% /></div></li></ul><h2 id="2-4-梯度下降算法"><a href="#2-4-梯度下降算法" class="headerlink" title="2.4 梯度下降算法"></a>2.4 梯度下降算法</h2><ul><li>算法实质与一元线性回归一致，表达形式近乎一致：&lt;/br&gt;<br>  <img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_GDAlgo.png" width = 70% /><ul><li>对于参数theta_j的计算：累加的是m个训练样本（特征向量）在当前预测函数h(x)下的预测值与实际值之差的平方和对第j个变量（特征）的导数</li><li>全部n+1个参数 theta_0, theta_1, … , theta_n 计算完成后再更新</li></ul></li><li>将其各项逐个写出即为：&lt;/br&gt;<br>  <img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/multiple_features_GDAlgo(1).png" width = 55% /></li></ul><h2 id="2-5-梯度下降的加速方法（tricks）"><a href="#2-5-梯度下降的加速方法（tricks）" class="headerlink" title="2.5 梯度下降的加速方法（tricks）"></a>2.5 梯度下降的加速方法（tricks）</h2><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><ul><li>将输入的数值规范为同样大小的规模，避免梯度下降过程中的反复震荡&lt;/br&gt;</li><li>将输入值除以输入变量的范围(即最大值减去最小值)，得到的新范围在-1到1之间。<br><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/feature_scaling.png" width = 100% /></li><li>作用：通过减少梯度下降的迭代次数从而起到加速作用<ul><li>注：不能解决梯度下降陷入局部最优解的问题<h3 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h3></li></ul></li><li>均值归一化是特征缩放的具体方法之一</li><li>操作如下：<ol><li>从每一输入变量的值中减去全体输入变量的平均值，从而得到一个平均值为0的新输入变量。</li><li>将新输入变量除以样本标准差</li></ol></li><li>遵循如下公式：&lt;/br&gt;<br>  <img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/mean_normalization.png" width = 30% />&lt;/br&gt;<br>  式中ui为所有变量i的值的平均值，si为变量i的值的范围（max-min）或标准差（standard deviation）</li></ul><h2 id="2-6-怎样选择合适的“学习率α”？"><a href="#2-6-怎样选择合适的“学习率α”？" class="headerlink" title="2.6 怎样选择合适的“学习率α”？"></a>2.6 怎样选择合适的“学习率α”？</h2><ul><li><strong>调试梯度下降</strong>：以迭代次数为x轴，代价函数J(theta)的值为y轴画出变化图。如果J(theta)一直在增长，那么应当将学习率减小</li><li><strong>自动收敛判断</strong>：若代价函数J(theta)在一次迭代中减少的值小于E（E是一个很小很小的值，例如0.001），那么可以判定J(theta)收敛了。但是在实际应用中选择合适的阈值E是非常困难的</li><li>已经证明：如果学习率α足够小,那么J(θ)将在每次迭代中都减少</li></ul><h1 id="三-自定义特征与多项式回归"><a href="#三-自定义特征与多项式回归" class="headerlink" title="三. 自定义特征与多项式回归"></a>三. 自定义特征与多项式回归</h1><p>我们可以将多个特征组合为一个，例如将特征x1, x2通过乘法运算x1*x2组合为新特征x3</p><h2 id="3-1-多项式回归"><a href="#3-1-多项式回归" class="headerlink" title="3.1 多项式回归"></a>3.1 多项式回归</h2><ul><li>若线性回归无法较好的拟合数据的分布，那么假设函数h(x)就需要是非线性的</li><li>可以通过将假设函数h(x)变成二次函数、三次函数或平方根函数(或任何其他形式)来改变曲线形状</li><li>注意： <strong>特征缩放！</strong><ul><li>平方和开方运算会较大地改变特征（变量）的取值范围</li><li>例如，x1∈(1,1000)，那么平方后x1^2∈(1,1000000)，立方后x1^3∈(1,1000000000)，开方后√x1∈(1,32)</li></ul></li></ul><h1 id="四-标准方程"><a href="#四-标准方程" class="headerlink" title="四. 标准方程"></a>四. 标准方程</h1><h2 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h2><ul><li>与梯度下降的作用一样，用于寻找使得代价函数J(theta)最小的theta值</li><li><strong>标准方程</strong>（normal equation）提供了一种高效的方法，可以直接解出theta</li><li>与梯度下降不同，标准方程不是一个迭代算法，而是通过分别对theta_j（每一个j）求导数并令其等于0求解得到。方程如下:<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/normal_equation.png" width = 30% /></div><br>  其中，X为m×(n+1)的矩阵，m行：每行为某个样本的特征向量，n+1列：n个特征 + 1个常数项因子（“1”）</li><li>使用标准方程的话无需对特征进行缩放</li></ul><h2 id="4-2-梯度下降VS标准方程"><a href="#4-2-梯度下降VS标准方程" class="headerlink" title="4.2 梯度下降VS标准方程"></a>4.2 梯度下降VS标准方程</h2><div class="table-container"><table><thead><tr><th style="text-align:center">梯度下降</th><th style="text-align:center">标准方程</th></tr></thead><tbody><tr><td style="text-align:center">需要选择学习率α</td><td style="text-align:center">无需对学习率α进行选择</td></tr><tr><td style="text-align:center">多次迭代</td><td style="text-align:center">没有迭代过程</td></tr><tr><td style="text-align:center">复杂度为O(k*n^2)</td><td style="text-align:center">复杂度为O(n^3)，需要计算矩阵(X^T)*X的逆</td></tr><tr><td style="text-align:center">算法在n非常大时表现稳定</td><td style="text-align:center">如果n非常大，那么算法执行时间会非常长</td></tr></tbody></table></div><ul><li>注：在实践中，当特征数n超过10,000时，可以考虑从标准方程法转为使用梯度下降。</li></ul><h2 id="4-3-不可逆性"><a href="#4-3-不可逆性" class="headerlink" title="4.3 不可逆性"></a>4.3 不可逆性</h2><ul><li>线性代数中，并非所有矩阵都是可逆的，我们将可逆矩阵称为奇异（singular）矩阵或退化（degenerate）矩阵</li><li>Octave/MATLAB中<ul><li>函数pinv()：pseudo-inverse 伪逆（一般使用此函数可以得到想求的theta）</li><li>函数inv()：inverse 逆</li></ul></li><li>什么情况下X^T*X不可逆？<ol><li>存在冗余特征（线性相关的）<ul><li>例如：在预测房价时，特征x_1为房子的面积，单位为feet^2，特征x_2为房子的面积，单位为m^2</li><li>因为1m = 3.28feet，故x_1 将恒等于 (3.28)^2 * x_2</li></ul></li><li>过量特征（e.g. 样本数量 ≤ 特征数量）<ul><li>解决方案：删除一部分特征 或 使用正则化（regularization）的方法</li></ul></li></ol></li></ul>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> linear regression </tag>
            
            <tag> gradient descent </tag>
            
            <tag> normal equation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-Theme-Snail</title>
      <link href="/2019/11/01/Hexo-Theme-Snail/"/>
      <url>/2019/11/01/Hexo-Theme-Snail/</url>
      
        <content type="html"><![CDATA[<h1 id="hexo-theme-snail"><a href="#hexo-theme-snail" class="headerlink" title="hexo-theme-snail"></a>hexo-theme-snail</h1><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">View Hexo-Theme-Snail Sources On Github &#10174; </a></p><p><a href="https://www.dusign.net" target="_blank" rel="noopener">View Live Super Snail Blog &#10174;</a></p><p><img src="snail.png" alt="hexo-theme-snail"></p><p>Hexo-theme-snail is a succinct hexo theme. It has two colors, light and star, that can be set according to your own preferences in the settings, and also has the functions of sharing and commenting. More features are under development.</p><h2 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h2><ul><li>light color theme and star theme</li><li>diversified comment system</li><li>notice tips</li><li>share to other platforms (under development)</li><li>picture sharing (under development)</li></ul><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a>Install Hexo</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-cli -g</span><br></pre></td></tr></table></figure><h3 id="Setup-your-blog"><a href="#Setup-your-blog" class="headerlink" title="Setup your blog"></a>Setup your blog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br></pre></td></tr></table></figure><h3 id="Installation-Theme"><a href="#Installation-Theme" class="headerlink" title="Installation Theme"></a>Installation Theme</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> blog</span><br><span class="line">$ rm -rf <span class="built_in">source</span></span><br><span class="line">$ rm _config.yml package.json README.md LICENSE</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/dusign/hexo-theme-snail.git</span><br><span class="line">$ mv ./hexo-theme-snail/snail ./themes</span><br><span class="line">$ mv ./hexo-theme-snail/* ./</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure><h3 id="Set-Theme"><a href="#Set-Theme" class="headerlink" title="Set Theme"></a>Set Theme</h3><p>Modify the value of <code>theme:</code> in <code>_config.yml</code><br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br></pre></td></tr></table></figure></p><h3 id="Start-the-Server"><a href="#Start-the-Server" class="headerlink" title="Start the Server"></a>Start the Server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><h3 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h3><p>Replace the following information with your own.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"><span class="attr">title:</span> </span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">At</span> <span class="string">the</span> <span class="string">bottom</span> <span class="string">of</span> <span class="string">the</span> <span class="string">well,</span> <span class="string">it</span> <span class="string">is</span> <span class="string">destined</span> <span class="string">to</span> <span class="string">see</span> <span class="string">only</span> <span class="string">the</span> <span class="string">sky</span> <span class="string">at</span> <span class="string">the</span> <span class="string">wellhead.</span> </span><br><span class="line">          <span class="string">However,</span> <span class="string">the</span> <span class="string">starting</span> <span class="string">point</span> <span class="string">only</span> <span class="string">affects</span> <span class="string">the</span> <span class="string">process</span> <span class="string">of</span> <span class="string">reaching</span> <span class="string">your</span> <span class="string">peak</span> <span class="string">and</span> <span class="string">does</span> <span class="string">not</span> <span class="string">determine</span> <span class="string">the</span> <span class="string">height</span> <span class="string">you</span> <span class="string">reach.</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Dusign</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">en</span></span><br><span class="line"><span class="attr">timezone:</span></span><br></pre></td></tr></table></figure></p><h3 id="Site-Settings"><a href="#Site-Settings" class="headerlink" title="Site Settings"></a>Site Settings</h3><p>Put customized pictures in <code>img</code> directory.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site settings</span></span><br><span class="line"><span class="attr">SEOTitle:</span> <span class="string">Hexo-theme-snail</span></span><br><span class="line"><span class="attr">email:</span> <span class="string">hexo-theme-snail@mail.com</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"A hexo theme"</span></span><br><span class="line"><span class="attr">keyword:</span> <span class="string">"dusign, hexo-theme-snail"</span></span><br><span class="line"><span class="attr">header-img:</span> <span class="string">img/header_img/home-bg-1-dark.jpg</span></span><br><span class="line"><span class="attr">signature:</span> <span class="literal">true</span> <span class="comment">#show signature</span></span><br><span class="line"><span class="attr">signature-img:</span> <span class="string">img/signature/Just-do-it-white.png</span></span><br></pre></td></tr></table></figure></p><h3 id="SNS-Settings"><a href="#SNS-Settings" class="headerlink" title="SNS Settings"></a>SNS Settings</h3><p>If you don’t want to display it, you can delete it directly.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SNS settings</span></span><br><span class="line"><span class="attr">github_username:</span>    <span class="string">dusign</span></span><br><span class="line"><span class="attr">twitter_username:</span>   <span class="string">dusignr</span></span><br><span class="line"><span class="attr">facebook_username:</span>  <span class="string">Gang</span> <span class="string">Du</span></span><br><span class="line"><span class="attr">zhihu_username:</span> <span class="string">dusignr</span></span><br></pre></td></tr></table></figure></p><h3 id="Sidebar-Settings"><a href="#Sidebar-Settings" class="headerlink" title="Sidebar Settings"></a>Sidebar Settings</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sidebar Settings</span></span><br><span class="line"><span class="attr">sidebar:</span> <span class="literal">true</span>                      <span class="comment"># whether or not using Sidebar.</span></span><br><span class="line"><span class="attr">sidebar-about-description:</span> <span class="string">"Welcome to visit, I'm Dusign!"</span></span><br><span class="line"><span class="attr">sidebar-avatar:</span> <span class="string">img/ironman-draw.png</span>      <span class="comment"># use absolute URL, seeing it's used in both `/` and `/about/`</span></span><br><span class="line"><span class="attr">widgets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">featured-tags</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">short-about</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">recent-posts</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">friends-blog</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">archive</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">category</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># widget behavior</span></span><br><span class="line"><span class="comment">## Archive</span></span><br><span class="line"><span class="attr">archive_type:</span> <span class="string">'monthly'</span></span><br><span class="line"><span class="attr">show_count:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Featured Tags</span></span><br><span class="line"><span class="attr">featured-tags:</span> <span class="literal">true</span>                     <span class="comment"># whether or not using Feature-Tags</span></span><br><span class="line"><span class="attr">featured-condition-size:</span> <span class="number">1</span>              <span class="comment"># A tag will be featured if the size of it is more than this condition value</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## Friends</span></span><br><span class="line"><span class="attr">friends:</span> <span class="string">[</span></span><br><span class="line">    <span class="string">&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Blog"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"https://blog.csdn.net/d_Nail"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Web"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"#"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Dusign's Github"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"https://github.com/dusign"</span></span><br><span class="line">    <span class="string">&#125;,&#123;</span></span><br><span class="line">        <span class="attr">title:</span> <span class="string">"Other"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">href:</span> <span class="string">"#"</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"><span class="string">]</span></span><br></pre></td></tr></table></figure><h3 id="Theme"><a href="#Theme" class="headerlink" title="Theme"></a>Theme</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">snail</span></span><br></pre></td></tr></table></figure><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">      <span class="attr">github:</span> <span class="string">github.repository.address</span></span><br><span class="line">      <span class="attr">coding:</span> <span class="string">coding.repository.address</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><h3 id="Comment"><a href="#Comment" class="headerlink" title="Comment"></a>Comment</h3><p>See httpymls://github.com/imsun/gitment for detailed configuration method.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Comment</span></span><br><span class="line"><span class="comment">## This comment system is gitment</span></span><br><span class="line"><span class="comment">## gitment url: https://github.com/imsun/gitment</span></span><br><span class="line"><span class="attr">comment:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">owner:</span></span><br><span class="line">  <span class="attr">repo:</span></span><br><span class="line">  <span class="attr">client_id:</span></span><br><span class="line">  <span class="attr">client_secret:</span></span><br></pre></td></tr></table></figure></p><h3 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tip</span></span><br><span class="line"><span class="attr">tip:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">content:</span> <span class="string">欢迎访问</span> <span class="string">&lt;a</span> <span class="string">href="https://www.dusign.net"</span> <span class="string">target="dusign"&gt;dusign&lt;/a&gt;</span> <span class="string">的博客，博客系统一键分享的功能还在完善中，请大家耐心等待。</span></span><br><span class="line">          <span class="string">若有问题或者有好的建议欢迎留言，笔者看到之后会及时回复。</span></span><br><span class="line">          <span class="string">评论点赞需要github账号登录，如果没有账号的话请点击</span> </span><br><span class="line">          <span class="string">&lt;a</span> <span class="string">href="https://github.com"</span> <span class="string">target="view_window"</span> <span class="string">&gt;</span> <span class="string">github</span> <span class="string">&lt;/a&gt;</span> <span class="string">注册，</span> <span class="string">谢谢</span> <span class="string">!</span></span><br></pre></td></tr></table></figure><h3 id="Color-Sheme"><a href="#Color-Sheme" class="headerlink" title="Color Sheme"></a>Color Sheme</h3><p>Set the <code>enable</code> value of the desired color sheme to <code>true</code>. If the value of <code>bg_effects.star.enable</code> is <code>true</code>, please modify the value of <code>highlight_theme</code> in <code>./themes/snail/_config.yml</code> to <code>night</code>.<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Color Sheme</span></span><br><span class="line"><span class="comment">## If there is no effect after modification, please empty the cache and try again.</span></span><br><span class="line"><span class="comment">## ⚠️ The following special effects will take up a lot of cpu resorces, please open it carefully.</span></span><br><span class="line"><span class="attr">bg_effects:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">line:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">color:</span> <span class="number">129</span><span class="string">,200,61</span></span><br><span class="line">    <span class="attr">pointColor:</span> <span class="number">129</span><span class="string">,200,61</span></span><br><span class="line">    <span class="attr">opacity:</span> <span class="number">0.7</span></span><br><span class="line">    <span class="attr">zIndex:</span> <span class="number">-9</span></span><br><span class="line">    <span class="attr">count:</span> <span class="number">99</span></span><br><span class="line">  <span class="attr">mouse_click:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">content:</span> <span class="string">'"🌱","just do it","🌾","🍀","don'</span><span class="string">'t give up","🍂","🌻","try it again","🍃","never say die","🌵","🌿","🌴"'</span></span><br><span class="line">    <span class="attr">color:</span> <span class="string">'"rgb(121,93,179)"</span></span><br><span class="line"><span class="string">          ,"rgb(76,180,231)"</span></span><br><span class="line"><span class="string">          ,"rgb(184,90,154)"</span></span><br><span class="line"><span class="string">          ,"rgb(157,211,250)"</span></span><br><span class="line"><span class="string">          ,"rgb(255,0,0)"</span></span><br><span class="line"><span class="string">          ,"rgb(242,153,29)"</span></span><br><span class="line"><span class="string">          ,"rgb(23,204,16)"</span></span><br><span class="line"><span class="string">          ,"rgb(222,0,0)"</span></span><br><span class="line"><span class="string">          ,"rgb(22,36,92)"</span></span><br><span class="line"><span class="string">          ,"rgb(127,24,116)"</span></span><br><span class="line"><span class="string">          ,"rgb(119,195,79)"</span></span><br><span class="line"><span class="string">          ,"rgb(4,77,34)"</span></span><br><span class="line"><span class="string">          ,"rgb(122,2,60)"'</span></span><br><span class="line">  <span class="attr">star:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><h2 id="Releases"><a href="#Releases" class="headerlink" title="Releases"></a>Releases</h2><p>V1.0</p><ul><li>fix the bugs</li><li>add comment system</li><li>add notice tips</li><li>add star sheme</li></ul><h2 id="License"><a href="#License" class="headerlink" title="License"></a>License</h2><p>Apache License 2.0 Copyright(c) 2018-2020 <a href="https://github.com/dusign" target="_blank" rel="noopener">Dusign</a>   </p><p><a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">hexo-theme-snail</a> is derived from <a href="https://github.com/Huxpro/huxpro.github.io" target="_blank" rel="noopener">Huxpro</a> Apache License 2.0. Copyright (c) 2015-2020 Huxpro</p>]]></content>
      
      
      
        <tags>
            
            <tag> dusign </tag>
            
            <tag> hexo-theme-snail </tag>
            
            <tag> snail </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
