<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A personal blog">
    <meta name="keyword"  content="chenlu, blog, homepage">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          Neural Network Tutorial - Lu CHEN
        
    </title>

    <link rel="canonical" href="https://dusign.net/2020/02/18/Neural-Network-Tutorial/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.0"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#machine learning" title="machine learning">machine learning</a>
                            
                              <a class="tag" href="/tags/#gradient descent" title="gradient descent">gradient descent</a>
                            
                              <a class="tag" href="/tags/#neural network" title="neural network">neural network</a>
                            
                              <a class="tag" href="/tags/#backpropagation" title="backpropagation">backpropagation</a>
                            
                        </div>
                        <h1>Neural Network Tutorial</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Jack on
                            2020-02-18
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">3.3k</span> and
                                Reading Time <span class="post-count">11</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Road Studio</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About me</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/photography/">Photography</a>
                        </li>
                        
                    

                    <li>
                        <a href="https://blog.csdn.net/d_Nail" target="_blank">Chinese Blog</a>
                    </li>
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="神经网络（Neural-Network）"><a href="#神经网络（Neural-Network）" class="headerlink" title="神经网络（Neural Network）"></a>神经网络（Neural Network）</h1><p>本文为提纲式笔记，对当前最热门的机器学习算法 —— 深度学习（Deep Learning, DL）的基础，神经网络，进行解析。主要介绍神经网络的数学模型与反向传播算法，重点对反向传播算法的原理、过程和实质进行探究，内容包括：神经网络模型表示、应用举例、反向传播算法的流程与实现。</p>
<blockquote>
<p>人工神经网络（Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。</p>
<p>神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。现代神经网络是一种 <strong>非线性统计性数据建模工具</strong>，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。</p>
</blockquote>
<p>和其他机器学习方法一样，神经网络已经被用于解决各种各样的问题，例如机器视觉和语音识别。这些问题都是很难被传统基于规则的编程所解决的。</p>
<h1 id="一-模型表示（Model-Representation）"><a href="#一-模型表示（Model-Representation）" class="headerlink" title="一. 模型表示（Model Representation）"></a>一. 模型表示（Model Representation）</h1><h2 id="1-1-数学模型"><a href="#1-1-数学模型" class="headerlink" title="1.1 数学模型"></a>1.1 数学模型</h2><ul>
<li>简单来讲，神经元（neurons）就是计算单元，它将输入模块（树突）获取电信号输入（称为“spikes”），并将其引导到输出模块（轴突）</li>
<li>在我们的模型中：<ul>
<li>轴突就是输入的特征 x_1, x_2, … , x_n，树突输出的就是假设函数的计算结果</li>
<li>输入单元 x_0 被称作“<strong>偏差单元</strong>”（bias unit），其值总是“1”</li>
<li>仍使用在逻辑回归中使用过的 S型(sigmoid)函数 作为假设函数，在神经网络中，通常称为“S型<strong>激活函数</strong>”（sigmoid activation function）</li>
<li>参数θ通常也被称为“<strong>权重</strong>”</li>
</ul>
</li>
<li>节点与层<ul>
<li>所有输入节点构成神经网络的第一层，通常称为“<strong>输入层</strong>”（Input layer）</li>
<li>最终输出假设函数结果的节点构成神经网络的最后一层，通常称为“<strong>输出层</strong>”（output layer）</li>
<li>输入层与输出层之间的层叫做“<strong>隐藏层</strong>”（hidden layers）</li>
</ul>
</li>
<li>举个例子：<ul>
<li>下面这张图中，我们将隐藏层中的节点记作a，上标代表该节点处于第几层，下标代表该节点是该层的第几个，并将其称为“<strong>激活单元</strong>”（activation units）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_example.png" width = 25% /></div></li>
<li>其中，各激活单元进行的是如下计算：（其中 <strong><em>Θ</em></strong> 表示相关的参数矩阵，上标表示该矩阵用于第几层的计算，下标表示矩阵的第几行第几列）<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_computation.png" width = 55% /></div></li>
<li>在本例中，我们用一个 3（下一层中除偏差单元以外的激活单元数目）× 4（该层中包含偏差单元的所有单元数目） 的参数矩阵来计算激活节点，将每一行的参数应用到输入中，以获得一个激活节点的值</li>
<li>假设函数的输出值取决于 1) 上一层激活节点值之和 与 2) 激活函数</li>
</ul>
</li>
</ul>
<h2 id="1-2-向量化（Vectorization）"><a href="#1-2-向量化（Vectorization）" class="headerlink" title="1.2 向量化（Vectorization）"></a>1.2 向量化（Vectorization）</h2><ul>
<li>下面，我们将继续用上面所举的例子，对计算式进行向量化：<ul>
<li>新定义向量 <strong><em>z</em></strong> ，上标代表节点所在层，下标表示节点在该层中的序号<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/one_hidden_layer_example_replace.png" width = 14% /></div></li>
<li>也就是说，向量 <strong><em>z</em></strong> 意味着：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vector_z.png" width = 40% /></div></li>
<li>当向量 <strong><em>x</em></strong> 是隐藏（激活）层 <strong><em>a</em></strong> 时，可将上式写为通式：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/vector_z_rewrite.png" width = 20% /></div></li>
<li>最后，输出层：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/neural_network_hypothesis_vectorized.png" width = 27% /></div>

</li>
</ul>
</li>
</ul>
<h2 id="1-3-应用举例（Applications）"><a href="#1-3-应用举例（Applications）" class="headerlink" title="1.3 应用举例（Applications）"></a>1.3 应用举例（Applications）</h2><h3 id="用简单神经网络模拟逻辑门"><a href="#用简单神经网络模拟逻辑门" class="headerlink" title="用简单神经网络模拟逻辑门"></a>用简单神经网络模拟逻辑门</h3><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/simple_nn_individual_example.png" width = 90% /></div>

<h3 id="用多层神经网络模拟复杂逻辑进行分类"><a href="#用多层神经网络模拟复杂逻辑进行分类" class="headerlink" title="用多层神经网络模拟复杂逻辑进行分类"></a>用多层神经网络模拟复杂逻辑进行分类</h3><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/simple_nn_mixed_example.png" width = 90% /></div>

<h2 id="1-4-多分类任务"><a href="#1-4-多分类任务" class="headerlink" title="1.4 多分类任务"></a>1.4 多分类任务</h2><p>在多分类任务中，我们有多个输出单元，最终输出的向量长度等于类别数量，且向量中只有一个元素为“1”，代表预测的类别，其他元素为“0”</p>
<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_multiple_output_oneVSall.png" width = 90% /></div>


<h1 id="二-代价函数与反向传播"><a href="#二-代价函数与反向传播" class="headerlink" title="二. 代价函数与反向传播"></a>二. 代价函数与反向传播</h1><h2 id="2-1-代价函数（Cost-Function）"><a href="#2-1-代价函数（Cost-Function）" class="headerlink" title="2.1 代价函数（Cost Function）"></a>2.1 代价函数（Cost Function）</h2><ul>
<li><p>规定以下记号：</p>
<ul>
<li><strong><em>L</em></strong>: 网络的总层数</li>
<li><strong><em>s_l</em></strong>：第 <em>l</em> 层中的节点数量（除去偏差单元）</li>
<li><strong><em>K</em></strong>：输出层的节点数（即类的数量）</li>
<li>因为神经网络可能有很多输出单元，我们记<strong><em>h(x)_k</em></strong>为假设函数结果的第k个值</li>
</ul>
</li>
<li><p>公式：</p>
<ul>
<li>基于逻辑回归的代价函数<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/regularized_logisticReg_costFunction.png" width=80%></div></li>
<li>改造为神经网络的代价函数<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_cost_function.png" width=85%></div></li>
</ul>
</li>
<li>对比：<ol>
<li>参数量的增加，由一个假设函数的参数向量<strong><em>θ</em></strong> ，到L-1层神经网络的参数矩阵<strong><em>Θ</em></strong> </li>
<li>公式的前半部分由一重累加和变为二重累加和，因为要将输出层中的K个单元的计算结果相加</li>
<li>公式的后半部分由一重累加和变为三重累加和，因为要将L-1层神经网络每层的参数矩阵的所有元素平方相加</li>
<li>注意：类似于逻辑回归，对常数项θ_0，我们想象为θ_0 * x_0，并且不对其进行正则化；在神经网络中对于每层参数矩阵的第0行第0列不进行正则化</li>
</ol>
</li>
</ul>
<h2 id="2-2-反向传播算法（Backpropagation）"><a href="#2-2-反向传播算法（Backpropagation）" class="headerlink" title="2.2 反向传播算法（Backpropagation）"></a>2.2 反向传播算法（Backpropagation）</h2><ul>
<li>注意：反向传播算法并非一个不同于梯度下降的算法，而是梯度下降算法运行在神经网络中时比较有效的计算方法</li>
<li>反向传播算法（Backpropagation）包括前向传播（Forward pass）和后向传播（Backward pass）两个过程，分别承担不同的计算任务</li>
<li>链式法则（chain rule）—— 反向传播算法的数学基础  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/chain_rule.png" width=45%></div></li>
<li>例子：（理解反向传播算法，李宏毅）<ul>
<li>假设有如下神经网络架构：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_nn.png" width=50%></div></li>
<li>对其输入层与第二层第一单元之间的关系进行分析：<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute.png" width=50%></div><br>计算 <strong>代价函数</strong> <strong><em>C</em></strong> 对 <strong>参数</strong> <strong><em>w</em></strong> 的导数，根据链式法则，可拆解为计算 <strong>函数</strong> <strong><em>z(w)</em></strong> 对 <strong>参数</strong> <strong><em>w</em></strong> 的导数与 <strong>代价函数</strong> <strong><em>C</em></strong> 对 <strong>变量</strong> <strong><em>z</em></strong> 的导数之积。其中，第一部分可以在 <strong>前向传播</strong> 的过程中直接得到：函数 <strong><em>z(w)</em></strong> 对 参数 <strong><em>w</em></strong> 的导数即为前一层网络中与参数w相关的变量 <strong><em>x</em></strong>。但是第二部分则需要在 <strong>后向传播</strong> 中计算</li>
<li>后向传播计算过程推导<br>考察下一层神经网络（假设所有神经元的激活函数均为sigmoid函数），如图：<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute1.png" width=70%></div><br>由链式法则，代价函数 <strong><em>C</em></strong> 对 变量 <strong><em>z</em></strong> 的导数可拆解为激活函数 <strong><em>a(z)</em></strong> 对 变量 <strong><em>z</em></strong> 的导数与代价函数 <strong><em>C</em></strong> 对 变量 <strong><em>a</em></strong> 的导数。其中，第一部分由激活函数的形式而确定；而第二部分，鉴于此神经元的计算结果（受参数w影响）参与到了下一层两个神经元的计算中，由链式法则，可以得出如图中最下方计算公式，简单整理后，在最右侧公式中，便可以发现递归计算的身影<br><div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_compute2.png" width=40%></div><br>通过以上推导过程，我们不难看出：类似于递归算法，若想计算代价函数对某一个参数（第1层中某个单元）的导数，必须先计算代价函数对第2层某个单元的导数，进而必须先计算代价函数对第3层某些(≥2)单元的导数，……，如此层层深入，先得出后面结果，否则无从计算前面。这就是 <strong>后向传播</strong> 的计算过程</li>
<li>后向传播在网络中的“计算流”<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_nn1.png" width=50%></div></li>
<li>小结：反向传播的两大过程 —— “前向传播”与“后向传播”<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_example_summary.png" width=50%></div></li>
</ul>
</li>
<li>算法过程<br>  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_algo.png" width=70%></div><ul>
<li>导入训练数据集，将神经网络每一层的偏差矩阵 <strong><em>∆^(l)</em></strong> 初始化为零矩阵</li>
<li>对于m个训练数据，依次进行：<ul>
<li>将特征向量输入第一层网络（输入层）</li>
<li>执行前向传播，计算并保存沿途各层的输出结果 <strong><em>a^(l), l = 2, 3, … , L</em></strong></li>
<li>计算网络输出与真实值的偏差向量 <strong><em>δ^(L)</em></strong></li>
<li>执行后向传播，计算并保存沿途各层的偏差向量 <strong><em>δ^(l), l = L-1, L-2, … , 2</em></strong>，向量化后得到如下式子：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_delta_partial_compute.png" width=30%></div>
  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_delta_vectorized.png" width=40%></div></li>
<li>更新各层偏差矩阵 <strong><em>∆^(l)</em></strong>，向量化公式如下：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/backpropagation_deltaCap_compute.png" width=30%></div></li>
</ul>
</li>
<li>计算代价函数对参数矩阵的偏导数<ul>
<li>矩阵 <strong><em>D^(l)</em></strong> 用作“累加器”，在计算的过程中把各个分部结果累加起来，最终计算出我们所求的偏导数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-3-反向传播算法实践"><a href="#2-3-反向传播算法实践" class="headerlink" title="2.3 反向传播算法实践"></a>2.3 反向传播算法实践</h2><h3 id="实现细节：参数展开（Unrolling-Parameters）"><a href="#实现细节：参数展开（Unrolling-Parameters）" class="headerlink" title="实现细节：参数展开（Unrolling Parameters）"></a>实现细节：参数展开（Unrolling Parameters）</h3><ul>
<li>不同于线性回归和逻辑回归，当我们使用神经网络时，参数的数学组织形式不再是向量，而是矩阵<ul>
<li><strong><em> Θ^(1), Θ^(2), Θ^(3), … </em></strong></li>
<li><strong><em> D^(1), D^(2), D^(3), … </em></strong></li>
</ul>
</li>
<li>为了能够使用 <strong><em>fminunc()</em></strong> 等高级优化算法，我们需要将以上矩阵“展开”成向量。例如，在Octave/MATLAB中使用以下命令：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">thetaVector &#x3D; [ Theta1(:); Theta2(:); Theta3(:); ]</span><br><span class="line">deltaVector &#x3D; [ D1(:); D2(:); D3(:) ]</span><br></pre></td></tr></table></figure></li>
<li>从“展开”后的向量变回矩阵（假设原矩阵Theta1的大小为10x11, Theta2的大小为10x11，Theta3的大小为1x11）：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Theta1 &#x3D; reshape(thetaVector(1:110),10,11)</span><br><span class="line">Theta2 &#x3D; reshape(thetaVector(111:220),10,11)</span><br><span class="line">Theta3 &#x3D; reshape(thetaVector(221:231),1,11)</span><br></pre></td></tr></table></figure></li>
<li>小节：  <div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/unroll_parameters.png" width=60%></div>

</li>
</ul>
<h3 id="实现细节：梯度检查（Gradient-Checking）"><a href="#实现细节：梯度检查（Gradient-Checking）" class="headerlink" title="实现细节：梯度检查（Gradient Checking）"></a>实现细节：梯度检查（Gradient Checking）</h3><ul>
<li>作用：检查神经网络反向传播算法的正确性 —— 是否正确计算梯度</li>
<li>由梯度的几何意义（切线），代价函数J(Θ)对特定参数Θ的偏导数可以计算如下：<ul>
<li>当只有一个参数矩阵时：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/derivative_approximation.png" width=35%></div></li>
<li>当有多个参数矩阵时，对Θ_j的偏导数：<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/derivative_approximation1.png" width=65%></div></li>
<li>其中，<strong><em>ε(epsilon)</em></strong> 为任意极小值，一般可以取10^(-4)，若太小会出现数值计算问题</li>
</ul>
</li>
<li>Octave/MATLAB实现：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epsilon &#x3D; 1e-4;</span><br><span class="line">for i &#x3D; 1:n,</span><br><span class="line"> 		thetaPlus &#x3D; theta;</span><br><span class="line"> 		thetaPlus(i) +&#x3D; epsilon;</span><br><span class="line"> 		thetaMinus &#x3D; theta;</span><br><span class="line"> 		thetaMinus(i) -&#x3D; epsilon;</span><br><span class="line"> 		gradApprox(i) &#x3D; (J(thetaPlus) - J(thetaMinus))&#x2F;(2*epsilon)</span><br><span class="line">end;</span><br></pre></td></tr></table></figure></li>
<li>一旦我们计算出gradApprox，与之前计算得到的deltaVector进行比较，若 gradApprox ≈ deltaVector 则说明反向传播算法实现正确</li>
<li>一旦验证了反向传播算法是正确的，就不需要再次计算gradApprox，因为gradApprox的计算代码运行很慢</li>
</ul>
<h3 id="实现细节：随机初始化（Random-Initialization）"><a href="#实现细节：随机初始化（Random-Initialization）" class="headerlink" title="实现细节：随机初始化（Random Initialization）"></a>实现细节：随机初始化（Random Initialization）</h3><ul>
<li>不同于线性回归和逻辑回归中将参数向量初始化为零向量，在神经网络中，参数矩阵不能初始化为零矩阵，因为这会使得隐藏层的各个单元输入均为零</li>
<li>而对参数矩阵的其他形式的初始化也不一定行得通，例如下图中的初始化方式 —— 来自同一神经元的联结参数相等（在图中使用同种颜色表示），会导致后面所有神经元在反向传播时的计算结果相同，即所有神经元计算同一函数，捕捉同一特征，使得神经网络“空有其表”，形成巨大冗余<div align=center><img src="https://raw.githubusercontent.com/Jack-CHEN-sci/Machine-Learning-Andrew/master/notes/img/nn_wrong_init.png" width=50%></div></li>
<li>因此，需要对神经网络的参数矩阵进行 <strong>随机初始化</strong>：将其赋值为区间[-ε,ε]中的随机数（注：此处的epsilon与梯度检查中的epsilon没有任何关系）</li>
<li>Octave/MATLAB实现：  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.</span><br><span class="line">% rand(x,y) is just a function in octave that will initialize </span><br><span class="line">% 	a matrix of random real numbers between 0 and 1.</span><br><span class="line"></span><br><span class="line">Theta1 &#x3D; rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta2 &#x3D; rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br><span class="line">Theta3 &#x3D; rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="实现小节（Summary）"><a href="#实现小节（Summary）" class="headerlink" title="实现小节（Summary）"></a>实现小节（Summary）</h3><ul>
<li>首先，选择一个神经网络架构：共有多少层、每层有多少个神经元 ……<ol>
<li>输入层单元数 = 特征向量的维度</li>
<li>输出层单元数 = 类别数量</li>
<li>隐藏层单元数： 通常情况下越多越好，但是需要权衡随着单元数量增加导致的计算成本增加</li>
<li>如果隐藏层的数量大于1，那么最好每层的神经元数量是相等的</li>
</ol>
</li>
<li>训练神经网络<ol>
<li>随机初始化参数（权重）</li>
<li>执行前向传播，对每个特征向量计算其预测值</li>
<li>计算代价函数</li>
<li>执行后向传播，计算梯度/偏导数</li>
<li>使用梯度检查，确保反向传播算法执行正确，然后关闭检查</li>
<li>使用梯度下降算法或其他高级优化算法最小化代价函数得到参数值</li>
</ol>
</li>
</ul>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2020/03/19/Estimating-Graphlet-Statistics-via-Random-Walk/" data-toggle="tooltip" data-placement="top" title="Estimating Graphlet Statistics via Random Walk">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2020/02/14/Logistic-Regression-Tutorial/" data-toggle="tooltip" data-placement="top" title="Logistic Regression Tutorial">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        This is copyright.
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#machine learning" title="machine learning">machine learning</a>
                        
                          <a class="tag" href="/tags/#gradient descent" title="gradient descent">gradient descent</a>
                        
                          <a class="tag" href="/tags/#neural network" title="neural network">neural network</a>
                        
                          <a class="tag" href="/tags/#backpropagation" title="backpropagation">backpropagation</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://www.sdu.edu.cn/" target="_blank">SDU</a></li>
                    
                        <li><a href="https://www.tsxt.sdu.edu.cn/" target="_blank">Taishan College</a></li>
                    
                        <li><a href="https://vislab.wang/" target="_blank">VisLab</a></li>
                    
                        <li><a href="https://idl.cs.washington.edu/" target="_blank">UW IDL</a></li>
                    
                        <li><a href="#" target="_blank">Other</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/Jack-CHEN-sci">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Jack 2020 
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=dusign&repo=hexo-theme-snail&type=star&count=true">
                    </iframe>
                    <br>
                    <img src="https://static.dy208.cn/o_1dfilp8ruo521thr1hvf18ji17soa.png">
                    <a href="http://www.beian.miit.gov.cn/"  style="color:#f72b07" target="_blank">鲁ICP备19044742号-1</a>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://dusign.net/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
